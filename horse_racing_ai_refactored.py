"""
ğŸ ç«¶é¦¬AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆ

æ©Ÿèƒ½:
- netkeiba.comã‹ã‚‰ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
- LightGBMãƒ™ãƒ¼ã‚¹ã®æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬
- æŠ•è³‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ä½¿ç”¨æ–¹æ³•:
1. HorseRacingPredictor.train_model() ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´
2. HorseRacingPredictor.predict_race(race_id) ã§äºˆæ¸¬å®Ÿè¡Œ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split
import requests
from bs4 import BeautifulSoup
import re
import time
import random
from datetime import datetime, timedelta
import pickle
from pathlib import Path
import optuna
from optuna.samplers import TPESampler
import json
import os

warnings.filterwarnings('ignore')

# è¨­å®š
plt.style.use('default')
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_palette("husl")

# å®šæ•°
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
]

PLACE_DICT = {
    'æœ­å¹Œ': '01', 'å‡½é¤¨': '02', 'ç¦å³¶': '03', 'æ–°æ½Ÿ': '04', 'æ±äº¬': '05',
    'ä¸­å±±': '06', 'ä¸­äº¬': '07', 'äº¬éƒ½': '08', 'é˜ªç¥': '09', 'å°å€‰': '10'
}

RACE_TYPE_DICT = {'èŠ': 'èŠ', 'ãƒ€': 'ãƒ€ãƒ¼ãƒˆ', 'éšœ': 'éšœå®³'}

def name_to_id(name):
    """é¦¬åã‚„é¨æ‰‹åãªã©ã®æ–‡å­—åˆ—ã‚’ç°¡æ˜“çš„ãªIDã«å¤‰æ›ã™ã‚‹ã€‚"""
    if not name or not isinstance(name, str):
        return '0'
    # æ–‡å­—åˆ—ã®å„æ–‡å­—ã®Unicodeã‚³ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®åˆè¨ˆã‚’IDã¨ã™ã‚‹
    return str(sum(ord(c) for c in name))


class ResultsAnalyzer:
    """çµæœã®åˆ†æãƒ»å¯è¦–åŒ–ãƒ»ä¿å­˜ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir='simulation_results'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # çµæœä¿å­˜ç”¨
        self.simulation_results = {}
        self.prediction_results = {}
        self.model_performance = {}
        
    def save_simulation_results(self, fukusho_gain, tansho_gain, test_data_info=None):
        """å›åç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’ä¿å­˜"""
        
        # çµæœãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        simulation_data = {
            'fukusho_results': fukusho_gain.to_dict(),
            'tansho_results': tansho_gain.to_dict(),
            'timestamp': self.timestamp,
            'test_data_info': test_data_info or {}
        }
        
        # JSONå½¢å¼ã§ä¿å­˜
        json_path = self.output_dir / f'simulation_results_{self.timestamp}.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(simulation_data, f, ensure_ascii=False, indent=2)
        
        # CSVå½¢å¼ã§ã‚‚ä¿å­˜
        csv_path = self.output_dir / f'simulation_results_{self.timestamp}.csv'
        combined_results = pd.DataFrame({
            'threshold': fukusho_gain.index,
            'fukusho_return_rate': fukusho_gain['return_rate'].values,
            'fukusho_std': fukusho_gain['std'].values,
            'fukusho_n_bets': fukusho_gain['n_bets'].values,
            'fukusho_n_hits': fukusho_gain['n_hits'].values,
            'tansho_return_rate': tansho_gain['return_rate'].values,
            'tansho_std': tansho_gain['std'].values,
            'tansho_n_bets': tansho_gain['n_bets'].values,
            'tansho_n_hits': tansho_gain['n_hits'].values,
        })
        combined_results.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # è©³ç´°ãªã‚°ãƒ©ãƒ•ã‚’ä½œæˆãƒ»ä¿å­˜
        self._create_detailed_simulation_plots(fukusho_gain, tansho_gain)
        
        print(f"âœ… ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœä¿å­˜å®Œäº†:")
        print(f"  JSON: {json_path}")
        print(f"  CSV: {csv_path}")
        
        return json_path, csv_path
    
    def _create_detailed_simulation_plots(self, fukusho_gain, tansho_gain):
        """è©³ç´°ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ"""
        
        # 1. åŸºæœ¬ã®å›åç‡ãƒ—ãƒ­ãƒƒãƒˆ
        plt.figure(figsize=(15, 10))
        
        # è¤‡å‹å›åç‡
        plt.subplot(2, 3, 1)
        plt.fill_between(fukusho_gain.index, 
                        y1=fukusho_gain['return_rate']-fukusho_gain['std'],
                        y2=fukusho_gain['return_rate']+fukusho_gain['std'],
                        alpha=0.3, color='blue')
        plt.plot(fukusho_gain.index, fukusho_gain['return_rate'], 'b-', linewidth=2, label='Place')
        plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Break-even Point')
        plt.title('Fukusho (Place) Return Rate (with Standard Deviation)')
        plt.xlabel('Threshold')
        plt.ylabel('Return Rate')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å˜å‹å›åç‡
        plt.subplot(2, 3, 2)
        plt.fill_between(tansho_gain.index, 
                        y1=tansho_gain['return_rate']-tansho_gain['std'],
                        y2=tansho_gain['return_rate']+tansho_gain['std'],
                        alpha=0.3, color='green')
        plt.plot(tansho_gain.index, tansho_gain['return_rate'], 'g-', linewidth=2, label='Win')
        plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Break-even Point')
        plt.title('Tansho (Win) Return Rate (with Standard Deviation)')
        plt.xlabel('Threshold')
        plt.ylabel('Return Rate')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æŠ•æ³¨æ•°ã®æ¨ç§»
        plt.subplot(2, 3, 3)
        plt.plot(fukusho_gain.index, fukusho_gain['n_bets'], 'b-', linewidth=2, label='Place Bets')
        plt.plot(tansho_gain.index, tansho_gain['n_bets'], 'g-', linewidth=2, label='Win Bets')
        plt.title('Number of Bets Trend')
        plt.xlabel('Threshold')
        plt.ylabel('Number of Bets')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # çš„ä¸­æ•°ã®æ¨ç§»
        plt.subplot(2, 3, 4)
        plt.plot(fukusho_gain.index, fukusho_gain['n_hits'], 'b-', linewidth=2, label='Place Wins')
        plt.plot(tansho_gain.index, tansho_gain['n_hits'], 'g-', linewidth=2, label='Win Hits')
        plt.title('Number of Wins Trend')
        plt.xlabel('Threshold')
        plt.ylabel('Number of Wins')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # çš„ä¸­ç‡ã®æ¨ç§»
        plt.subplot(2, 3, 5)
        fukusho_hit_rate = fukusho_gain['n_hits'] / fukusho_gain['n_bets'].replace(0, 1)
        tansho_hit_rate = tansho_gain['n_hits'] / tansho_gain['n_bets'].replace(0, 1)
        plt.plot(fukusho_gain.index, fukusho_hit_rate, 'b-', linewidth=2, label='Place Hit Rate')
        plt.plot(tansho_gain.index, tansho_hit_rate, 'g-', linewidth=2, label='Win Hit Rate')
        plt.title('Win Rate Trend')
        plt.xlabel('Threshold')
        plt.ylabel('Win Rate')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
        plt.subplot(2, 3, 6)
        plt.plot(fukusho_gain.index, fukusho_gain['return_rate'], 'b-', linewidth=2, label='Place Return Rate')
        plt.plot(tansho_gain.index, tansho_gain['return_rate'], 'g-', linewidth=2, label='Win Return Rate')
        plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Break-even Point')
        plt.fill_between(fukusho_gain.index, 0.8, 1.2, alpha=0.1, color='red', label='Â±20% Zone')
        plt.title('Return Rate Comparison')
        plt.xlabel('Threshold')
        plt.ylabel('Return Rate')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ç”»åƒä¿å­˜
        plot_path = self.output_dir / f'simulation_analysis_{self.timestamp}.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚‚ä½œæˆ
        self._create_summary_report(fukusho_gain, tansho_gain)
        
        print(f"  ğŸ“Š è©³ç´°ã‚°ãƒ©ãƒ•: {plot_path}")
    
    def _create_summary_report(self, fukusho_gain, tansho_gain):
        """çµ±è¨ˆã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ"""
        
        # æœ€é©é–¾å€¤ã®è¨ˆç®—
        best_fukusho_threshold = fukusho_gain.loc[fukusho_gain['return_rate'].idxmax()]
        best_tansho_threshold = tansho_gain.loc[tansho_gain['return_rate'].idxmax()]
        
        # æç›Šåˆ†å²ç‚¹ã«æœ€ã‚‚è¿‘ã„é–¾å€¤
        fukusho_breakeven = fukusho_gain.iloc[(fukusho_gain['return_rate'] - 1.0).abs().argsort()[:1]]
        tansho_breakeven = tansho_gain.iloc[(tansho_gain['return_rate'] - 1.0).abs().argsort()[:1]]
        
        report = f"""
# ç«¶é¦¬AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## ğŸ“Š è¤‡å‹æŠ•è³‡æˆ¦ç•¥åˆ†æ

### æœ€é«˜å›åç‡
- é–¾å€¤: {best_fukusho_threshold.name:.3f}
- å›åç‡: {best_fukusho_threshold['return_rate']:.3f} ({(best_fukusho_threshold['return_rate']-1)*100:+.1f}%)
- æŠ•æ³¨æ•°: {best_fukusho_threshold['n_bets']:.0f}å›
- çš„ä¸­æ•°: {best_fukusho_threshold['n_hits']:.0f}å›
- çš„ä¸­ç‡: {best_fukusho_threshold['n_hits']/best_fukusho_threshold['n_bets']*100:.1f}%
- æ¨™æº–åå·®: {best_fukusho_threshold['std']:.3f}

### æç›Šåˆ†å²ç‚¹ä»˜è¿‘
- é–¾å€¤: {fukusho_breakeven.index[0]:.3f}
- å›åç‡: {fukusho_breakeven['return_rate'].iloc[0]:.3f}
- æŠ•æ³¨æ•°: {fukusho_breakeven['n_bets'].iloc[0]:.0f}å›

## ğŸ¯ å˜å‹æŠ•è³‡æˆ¦ç•¥åˆ†æ

### æœ€é«˜å›åç‡
- é–¾å€¤: {best_tansho_threshold.name:.3f}
- å›åç‡: {best_tansho_threshold['return_rate']:.3f} ({(best_tansho_threshold['return_rate']-1)*100:+.1f}%)
- æŠ•æ³¨æ•°: {best_tansho_threshold['n_bets']:.0f}å›
- çš„ä¸­æ•°: {best_tansho_threshold['n_hits']:.0f}å›
- çš„ä¸­ç‡: {best_tansho_threshold['n_hits']/best_tansho_threshold['n_bets']*100:.1f}%
- æ¨™æº–åå·®: {best_tansho_threshold['std']:.3f}

### æç›Šåˆ†å²ç‚¹ä»˜è¿‘
- é–¾å€¤: {tansho_breakeven.index[0]:.3f}
- å›åç‡: {tansho_breakeven['return_rate'].iloc[0]:.3f}
- æŠ•æ³¨æ•°: {tansho_breakeven['n_bets'].iloc[0]:.0f}å›

## ğŸ’¡ æŠ•è³‡æˆ¦ç•¥ã®æ¨å¥¨

### è¤‡å‹æˆ¦ç•¥
- {'ğŸ”¥ é«˜åç›Šæˆ¦ç•¥' if best_fukusho_threshold['return_rate'] > 1.1 else 'ğŸ“ˆ å®‰å®šæˆ¦ç•¥' if best_fukusho_threshold['return_rate'] > 1.0 else 'âš ï¸ è¦æ³¨æ„'}
- æ¨å¥¨é–¾å€¤: {best_fukusho_threshold.name:.3f}
- æœŸå¾…å›åç‡: {best_fukusho_threshold['return_rate']:.3f}

### å˜å‹æˆ¦ç•¥  
- {'ğŸ”¥ é«˜åç›Šæˆ¦ç•¥' if best_tansho_threshold['return_rate'] > 1.1 else 'ğŸ“ˆ å®‰å®šæˆ¦ç•¥' if best_tansho_threshold['return_rate'] > 1.0 else 'âš ï¸ è¦æ³¨æ„'}
- æ¨å¥¨é–¾å€¤: {best_tansho_threshold.name:.3f}
- æœŸå¾…å›åç‡: {best_tansho_threshold['return_rate']:.3f}

## âš ï¸ ãƒªã‚¹ã‚¯åˆ†æ
- è¤‡å‹æœ€å¤§æ¨™æº–åå·®: {fukusho_gain['std'].max():.3f}
- å˜å‹æœ€å¤§æ¨™æº–åå·®: {tansho_gain['std'].max():.3f}
- æŠ•è³‡æ¨å¥¨åº¦: {'é«˜' if min(best_fukusho_threshold['return_rate'], best_tansho_threshold['return_rate']) > 1.05 else 'ä¸­' if min(best_fukusho_threshold['return_rate'], best_tansho_threshold['return_rate']) > 1.0 else 'ä½'}

---
ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ç«¶é¦¬AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚
"""
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = self.output_dir / f'analysis_report_{self.timestamp}.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"  ğŸ“ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    
    def save_prediction_results(self, race_id, prediction_data, race_info=None):
        """äºˆæ¸¬çµæœã‚’ä¿å­˜"""
        
        prediction_record = {
            'race_id': race_id,
            'timestamp': datetime.now().isoformat(),
            'race_info': race_info or {},
            'predictions': prediction_data.to_dict('records') if hasattr(prediction_data, 'to_dict') else prediction_data
        }
        
        # äºˆæ¸¬çµæœã‚’JSONã§ä¿å­˜
        pred_path = self.output_dir / f'prediction_{race_id}_{self.timestamp}.json'
        with open(pred_path, 'w', encoding='utf-8') as f:
            json.dump(prediction_record, f, ensure_ascii=False, indent=2)
        
        # CSVå½¢å¼ã§ã‚‚ä¿å­˜
        if hasattr(prediction_data, 'to_csv'):
            csv_path = self.output_dir / f'prediction_{race_id}_{self.timestamp}.csv'
            prediction_data.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"âœ… äºˆæ¸¬çµæœä¿å­˜: {csv_path}")
        
        return pred_path
    
    def save_model_performance(self, performance_metrics, feature_importance=None, best_params=None):
        """ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æŒ‡æ¨™ã‚’ä¿å­˜"""
        
        model_data = {
            'timestamp': self.timestamp,
            'performance_metrics': performance_metrics,
            'feature_importance': feature_importance.to_dict('records') if feature_importance is not None else None,
            'best_params': best_params
        }
        
        # æ€§èƒ½ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        perf_path = self.output_dir / f'model_performance_{self.timestamp}.json'
        with open(perf_path, 'w', encoding='utf-8') as f:
            json.dump(model_data, f, ensure_ascii=False, indent=2)
        
        # ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–
        if feature_importance is not None:
            self._plot_feature_importance(feature_importance)
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ä¿å­˜: {perf_path}")
        return perf_path
    
    def _plot_feature_importance(self, feature_importance):
        """ç‰¹å¾´é‡é‡è¦åº¦ã®ãƒ—ãƒ­ãƒƒãƒˆ"""
        
        plt.figure(figsize=(12, 8))
        
        # ãƒˆãƒƒãƒ—20ã®ç‰¹å¾´é‡ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        top_features = feature_importance.head(20)
        
        plt.barh(range(len(top_features)), top_features['importance'].values, color='skyblue')
        plt.yticks(range(len(top_features)), top_features['feature'].values)
        plt.xlabel('Importance (Gain)')
        plt.title('Feature Importance Top 20')
        plt.gca().invert_yaxis()
        
        # å€¤ã‚’ãƒãƒ¼ã«è¡¨ç¤º
        for i, v in enumerate(top_features['importance'].values):
            plt.text(v + max(top_features['importance']) * 0.01, i, f'{v:.0f}', 
                    va='center', fontsize=9)
        
        plt.tight_layout()
        
        # ä¿å­˜
        importance_path = self.output_dir / f'feature_importance_{self.timestamp}.png'
        plt.savefig(importance_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ã‚°ãƒ©ãƒ•: {importance_path}")
    
    def create_comprehensive_report(self):
        """åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
        
        # å…¨çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§
        json_files = list(self.output_dir.glob('*.json'))
        csv_files = list(self.output_dir.glob('*.csv'))
        png_files = list(self.output_dir.glob('*.png'))
        
        summary_report = f"""
# ğŸ ç«¶é¦¬AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆ
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

### JSONå½¢å¼ãƒ‡ãƒ¼ã‚¿ ({len(json_files)}ä»¶)
{chr(10).join([f'- {f.name}' for f in json_files])}

### CSVå½¢å¼ãƒ‡ãƒ¼ã‚¿ ({len(csv_files)}ä»¶)
{chr(10).join([f'- {f.name}' for f in csv_files])}

### å¯è¦–åŒ–ç”»åƒ ({len(png_files)}ä»¶)
{chr(10).join([f'- {f.name}' for f in png_files])}

## ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•

### Pythonã§ã®èª­ã¿è¾¼ã¿ä¾‹
```python
import pandas as pd
import json

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
sim_data = pd.read_csv('{self.output_dir}/simulation_results_YYYYMMDD_HHMMSS.csv')

# äºˆæ¸¬çµæœ
with open('{self.output_dir}/prediction_RACEID_YYYYMMDD_HHMMSS.json', 'r', encoding='utf-8') as f:
    pred_data = json.load(f)

# ãƒ¢ãƒ‡ãƒ«æ€§èƒ½
with open('{self.output_dir}/model_performance_YYYYMMDD_HHMMSS.json', 'r', encoding='utf-8') as f:
    model_data = json.load(f)
```

---
ç·åˆãƒ¬ãƒãƒ¼ãƒˆ by ç«¶é¦¬AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
"""
        
        # åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        comprehensive_path = self.output_dir / f'comprehensive_report_{self.timestamp}.md'
        with open(comprehensive_path, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        print(f"ğŸ“‹ åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†: {comprehensive_path}")
        return comprehensive_path


class Return:
    """æ‰•ã„æˆ»ã—è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, return_tables):
        self.return_tables = return_tables
    
    @classmethod
    def read_pickle(cls, path_list):
        """pickleãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ‰•ã„æˆ»ã—è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        df = pd.read_pickle(path_list[0])
        for path in path_list[1:]:
            df = pd.concat([df, pd.read_pickle(path)])
        return cls(df)
    
    @property
    def fukusho(self):
        """è¤‡å‹ã®æ‰•ã„æˆ»ã—è¡¨ã‚’å–å¾—"""
        fukusho = self.return_tables[self.return_tables[0]=='è¤‡å‹'][[1,2]]
        wins = fukusho[1].str.split('br', expand=True)[[0,1,2]]
        
        wins.columns = ['win_0', 'win_1', 'win_2']
        returns = fukusho[2].str.split('br', expand=True)[[0,1,2]]
        returns.columns = ['return_0', 'return_1', 'return_2']
        
        df = pd.concat([wins, returns], axis=1)
        for column in df.columns:
            df[column] = df[column].str.replace(',', '')
        return df.fillna(0).astype(int)
    
    @property
    def tansho(self):
        """å˜å‹ã®æ‰•ã„æˆ»ã—è¡¨ã‚’å–å¾—"""
        tansho = self.return_tables[self.return_tables[0]=='å˜å‹'][[1,2]]
        tansho.columns = ['win', 'return']
        
        for column in tansho.columns:
            tansho[column] = pd.to_numeric(tansho[column], errors='coerce')
            
        return tansho


class HorseResults:
    """é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, horse_results):
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã«åˆã‚ã›ã¦ä¿®æ­£
        target_columns = ['date', 'rank', 'prize', 'diff', 'passing', 'venue', 'distance']
        available_columns = [col for col in target_columns if col in horse_results.columns]
        self.horse_results = horse_results[available_columns]
        self.preprocessing()
    
    @classmethod
    def read_pickle(cls, path_list):
        """pickleãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        df = pd.read_pickle(path_list[0])
        for path in path_list[1:]:
            df = DataProcessor.update_data(df, pd.read_pickle(path))
        return cls(df)
    
    @staticmethod
    def scrape(horse_id_list):
        """é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°"""
        horse_results = {}
        for horse_id in tqdm(horse_id_list):
            time.sleep(1)
            try:
                url = 'https://db.netkeiba.com/horse/' + horse_id
                headers = {'User-Agent': random.choice(USER_AGENTS)}
                html = requests.get(url, headers=headers)
                html.encoding = "EUC-JP"
                df = pd.read_html(html.text)[2]
                df.index = [horse_id] * len(df)
                horse_results[horse_id] = df
            except (IndexError, Exception):
                continue

        # pd.DataFrameå‹ã«ã—ã¦ä¸€ã¤ã®ãƒ‡ãƒ¼ã‚¿ã«ã¾ã¨ã‚ã‚‹        
        horse_results_df = pd.concat([horse_results[key] for key in horse_results])
        return horse_results_df
    
    def preprocessing(self):
        """å‰å‡¦ç†ã‚’å®Ÿè¡Œ"""
        df = self.horse_results.copy()

        # ç€é †ã®æ•°å€¤åŒ–
        df['rank'] = pd.to_numeric(df['rank'], errors='coerce')
        df.dropna(subset=['rank'], inplace=True)
        df['rank'] = df['rank'].astype(int)

        # æ—¥ä»˜å¤‰æ›
        df["date"] = pd.to_datetime(df["date"])
        
        # è³é‡‘ã®æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
        if 'prize' in df.columns:
            df['prize'] = pd.to_numeric(df['prize'], errors='coerce')
            df['prize'].fillna(0, inplace=True)
        else:
            df['prize'] = 0
        
        # ç€å·®ã®å‡¦ç†
        if 'diff' in df.columns:
            df['diff'] = pd.to_numeric(df['diff'], errors='coerce')
            df['diff'] = df['diff'].map(lambda x: 0 if pd.isna(x) or x < 0 else x)
        else:
            df['diff'] = 0
        
        # ãƒ¬ãƒ¼ã‚¹å±•é–‹ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        if 'passing' in df.columns:
            def corner(x, n):
                if pd.isna(x) or type(x) != str:
                    return 0
                numbers = re.findall(r'\d+', str(x))
                if len(numbers) == 0:
                    return 0
                elif n == 4:
                    return int(numbers[-1]) if numbers else 0
                elif n == 1:
                    return int(numbers[0]) if numbers else 0
                    
            df['first_corner'] = df['passing'].map(lambda x: corner(x, 1))
            df['final_corner'] = df['passing'].map(lambda x: corner(x, 4))
        else:
            df['first_corner'] = 0
            df['final_corner'] = 0
        
        # å±•é–‹æŒ‡æ¨™ã®è¨ˆç®—
        df['final_to_rank'] = df['final_corner'] - df['rank']
        df['first_to_rank'] = df['first_corner'] - df['rank']
        df['first_to_final'] = df['first_corner'] - df['final_corner']
        
        # é–‹å‚¬å ´æ‰€ã®å‡¦ç†
        if 'venue' in df.columns:
            venue_dict = PLACE_DICT
            df['é–‹å‚¬'] = df['venue'].map(venue_dict).fillna('11')
        else:
            df['é–‹å‚¬'] = '11'
        
        # ãƒ¬ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®å‡¦ç†
        if 'distance' in df.columns:
            df['distance_str'] = df['distance'].astype(str)
            df['race_type'] = df['distance_str'].apply(lambda x: 
                'èŠ' if 'T' in str(x) or 'èŠ' in str(x) else 
                'ãƒ€ãƒ¼ãƒˆ' if 'D' in str(x) or 'ãƒ€' in str(x) else 'èŠ')
            
            distance_numbers = df['distance_str'].str.extract(r'(\d+)', expand=False)
            df['course_len'] = pd.to_numeric(distance_numbers, errors='coerce').fillna(1600) // 100
            df.drop('distance_str', axis=1, inplace=True)
        else:
            df['race_type'] = 'èŠ'
            df['course_len'] = 16
        
        df.index.name = 'horse_id'
        
        self.horse_results = df
        self.target_list = ['rank', 'prize', 'diff', 'first_corner', 'final_corner',
                           'first_to_rank', 'first_to_final', 'final_to_rank']
    
    def average(self, horse_id_list, date, n_samples='all'):
        """æŒ‡å®šã—ãŸé¦¬ã®éå»æˆç¸¾ã®å¹³å‡ã‚’è¨ˆç®—"""
        target_df = self.horse_results.query('index in @horse_id_list')
        
        if n_samples == 'all':
            filtered_df = target_df[target_df['date'] < date]
        elif n_samples > 0:
            filtered_df = target_df[target_df['date'] < date]\
                .sort_values('date', ascending=False).groupby(level=0).head(n_samples)
        else:
            raise Exception('n_samples must be >0')
        
        self.average_dict = {}
        self.average_dict['non_category'] = filtered_df.groupby(level=0)[self.target_list].mean()\
            .add_suffix('_{}R'.format(n_samples))
            
        for column in ['course_len', 'race_type', 'é–‹å‚¬']:
            if column in filtered_df.columns:
                self.average_dict[column] = filtered_df.groupby(['horse_id', column])\
                    [self.target_list].mean().add_suffix('_{}_{}R'.format(column, n_samples))
            else:
                self.average_dict[column] = pd.DataFrame()

        if n_samples == 5:
            self.latest = filtered_df.groupby('horse_id')['date'].max().rename('latest')
    
    def merge(self, results, date, n_samples='all'):
        """æŒ‡å®šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ã«éå»æˆç¸¾ã‚’çµåˆ"""
        df = results[results['date'] == date]
        horse_id_list = df['horse_id']
        self.average(horse_id_list, date, n_samples)
        
        merged_df = df.merge(self.average_dict['non_category'], 
                           left_on='horse_id', right_index=True, how='left')
        
        for column in ['course_len', 'race_type', 'é–‹å‚¬']:
            if not self.average_dict[column].empty:
                merged_df = merged_df.merge(self.average_dict[column], 
                                          left_on=['horse_id', column],
                                          right_index=True, how='left')

        if n_samples == 5 and hasattr(self, 'latest'):
            merged_df = merged_df.merge(self.latest, left_on='horse_id',
                                      right_index=True, how='left')
        return merged_df
    
    def merge_all(self, results, n_samples='all'):
        """å…¨æ—¥ç¨‹ã®ãƒ‡ãƒ¼ã‚¿ã«éå»æˆç¸¾ã‚’çµåˆ"""
        date_list = results['date'].unique()
        merged_df = pd.concat([self.merge(results, date, n_samples) 
                              for date in tqdm(date_list)])
        return merged_df


class Peds:
    """è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, peds):
        self.peds = peds
        self.peds_e = pd.DataFrame()
    
    @classmethod
    def read_pickle(cls, path_list):
        """pickleãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        df = pd.read_pickle(path_list[0])
        for path in path_list[1:]:
            df = DataProcessor.update_data(df, pd.read_pickle(path))
        return cls(df)
    
    def encode(self):
        """è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        df = self.peds.copy()
        for column in df.columns:
            df[column] = LabelEncoder().fit_transform(df[column].fillna('Na'))
        self.peds_e = df.astype('category')


class ShutubaTable:
    """å‡ºé¦¬è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ã‚¯ãƒ©ã‚¹ï¼ˆäºˆæ¸¬å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰"""
    
    def __init__(self, shutuba_tables):
        self.data = shutuba_tables
        self.data_p = pd.DataFrame()
        self.data_h = pd.DataFrame()
        self.data_pe = pd.DataFrame()
        self.data_c = pd.DataFrame()
        
    @classmethod
    def scrape(cls, race_id_list, date):
        """å‡ºé¦¬è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹"""
        data = pd.DataFrame()
        successful_races = 0
        
        for race_id in tqdm(race_id_list):
            time.sleep(1)
            try:
                url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id
                headers = {'User-Agent': random.choice(USER_AGENTS)}
                html = requests.get(url, headers=headers)
                html.encoding = "EUC-JP"

                if html.status_code != 200:
                    print(f"ãƒ¬ãƒ¼ã‚¹ID {race_id}: ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—")
                    continue

                df = pd.read_html(html.text)[0]
                df = df.rename(columns=lambda x: x.replace(' ', ''))
                df = df.T.reset_index(level=0, drop=True).T

                soup = BeautifulSoup(html.text, "html.parser")

                race_data_div = soup.find('div', attrs={'class': 'RaceData01'})
                if race_data_div is None:
                    print(f"ãƒ¬ãƒ¼ã‚¹ID {race_id}: ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    continue

                texts = race_data_div.text
                texts = re.findall(r'\w+', texts)
                for text in texts:
                    if 'm' in text:
                        df['course_len'] = [int(re.findall(r'\d+', text)[-1])] * len(df)
                    if text in ["æ›‡", "æ™´", "é›¨", "å°é›¨", "å°é›ª", "é›ª"]:
                        df["weather"] = [text] * len(df)
                    if text in ["è‰¯", "ç¨é‡", "é‡"]:
                        df["ground_state"] = [text] * len(df)
                    if 'ä¸' in text:
                        df["ground_state"] = ['ä¸è‰¯'] * len(df)
                    if 'ç¨' in text:
                        df["ground_state"] = ['ç¨é‡'] * len(df)
                    if 'èŠ' in text:
                        df['race_type'] = ['èŠ'] * len(df)
                    if 'éšœ' in text:
                        df['race_type'] = ['éšœå®³'] * len(df)
                    if 'ãƒ€' in text:
                        df['race_type'] = ['ãƒ€ãƒ¼ãƒˆ'] * len(df)
                df['date'] = [date] * len(df)

                # horse_id
                horse_id_list = []
                horse_td_list = soup.find_all("td", attrs={'class': 'HorseInfo'})
                for td in horse_td_list:
                    horse_link = td.find('a')
                    if horse_link and 'href' in horse_link.attrs:
                        horse_id = re.findall(r'\d+', horse_link['href'])[0]
                        horse_id_list.append(horse_id)

                # jockey_id
                jockey_id_list = []
                jockey_td_list = soup.find_all("td", attrs={'class': 'Jockey'})
                for td in jockey_td_list:
                    jockey_link = td.find('a')
                    if jockey_link and 'href' in jockey_link.attrs:
                        jockey_id = re.findall(r'\d+', jockey_link['href'])[0]
                        jockey_id_list.append(jockey_id)

                if len(horse_id_list) != len(df) or len(jockey_id_list) != len(df):
                    print(f"ãƒ¬ãƒ¼ã‚¹ID {race_id}: ãƒ‡ãƒ¼ã‚¿ã®ä¸æ•´åˆ")
                    continue

                df['horse_id'] = horse_id_list
                df['jockey_id'] = jockey_id_list

                df.index = [race_id] * len(df)
                data = pd.concat([data, df])
                successful_races += 1
                print(f"ãƒ¬ãƒ¼ã‚¹ID {race_id}: å–å¾—æˆåŠŸ ({len(df)}é ­)")
                
            except Exception as e:
                print(f"ãƒ¬ãƒ¼ã‚¹ID {race_id}: ã‚¨ãƒ©ãƒ¼ - {e}")
                continue
        
        print(f"å–å¾—å®Œäº†: {successful_races}/{len(race_id_list)} ãƒ¬ãƒ¼ã‚¹")
        return cls(data)
             
    def preprocessing(self):
        """å‰å‡¦ç†ã‚’å®Ÿè¡Œ"""
        df = self.data.copy()
        
        if df.empty:
            print("è­¦å‘Š: å‡ºé¦¬è¡¨ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            self.data_p = pd.DataFrame()
            return
        
        # å¿…è¦ãªåˆ—ã®ç¢ºèªã¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
        if "æ€§é½¢" not in df.columns:
            df["æ€§é½¢"] = "ç‰¡4"
        if "é¦¬ä½“é‡(å¢—æ¸›)" not in df.columns:
            df["é¦¬ä½“é‡(å¢—æ¸›)"] = "500(0)"
        
        df["æ€§"] = df["æ€§é½¢"].map(lambda x: str(x)[0])
        df["å¹´é½¢"] = df["æ€§é½¢"].map(lambda x: str(x)[1:]).astype(int)

        df = df[df["é¦¬ä½“é‡(å¢—æ¸›)"] != '--']
        if not df.empty:
            df["ä½“é‡"] = df["é¦¬ä½“é‡(å¢—æ¸›)"].str.split("(", expand=True)[0].astype(int)
            df["ä½“é‡å¤‰åŒ–"] = df["é¦¬ä½“é‡(å¢—æ¸›)"].str.split("(", expand=True)[1].str[:-1]
            df['ä½“é‡å¤‰åŒ–'] = pd.to_numeric(df['ä½“é‡å¤‰åŒ–'], errors='coerce')
        
        df["date"] = pd.to_datetime(df["date"])
        
        if 'æ ' in df.columns:
            df['æ '] = df['æ '].astype(int)
        if 'é¦¬ç•ª' in df.columns:
            df['é¦¬ç•ª'] = df['é¦¬ç•ª'].astype(int)
        if 'æ–¤é‡' in df.columns:
            df['æ–¤é‡'] = df['æ–¤é‡'].astype(int)
            
        df['é–‹å‚¬'] = df.index.map(lambda x:str(x)[4:6])
        df['n_horses'] = df.index.map(df.index.value_counts())

        if 'course_len' in df.columns:
            df["course_len"] = df["course_len"].astype(float) // 100
        else:
            df["course_len"] = 16

        if 'weather' not in df.columns:
            df['weather'] = 'æ™´'
        if 'race_type' not in df.columns:
            df['race_type'] = 'èŠ'
        if 'ground_state' not in df.columns:
            df['ground_state'] = 'è‰¯'

        available_cols = ['æ ', 'é¦¬ç•ª', 'æ–¤é‡', 'course_len', 'weather','race_type',
        'ground_state', 'date', 'horse_id', 'jockey_id', 'æ€§', 'å¹´é½¢',
       'ä½“é‡', 'ä½“é‡å¤‰åŒ–', 'é–‹å‚¬', 'n_horses']
        
        cols_to_use = [col for col in available_cols if col in df.columns]
        df = df[cols_to_use]
        
        self.data_p = df.rename(columns={'æ ': 'æ ç•ª'}) if 'æ ' in df.columns else df

    def merge_horse_results(self, hr, n_samples_list=[5, 9, 'all']):
        """é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ"""
        self.data_h = self.data_p.copy()
        for n_samples in n_samples_list:
            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)
            
        self.data_h['interval'] = (self.data_h['date'] - self.data_h['latest']).dt.days
        self.data_h.drop(['é–‹å‚¬', 'latest'], axis=1, inplace=True, errors='ignore')
        
    def merge_peds(self, peds):
        """è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ"""
        self.data_pe = self.data_h.merge(peds, left_on='horse_id', right_index=True, how='left')
        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]['horse_id'].unique()
        if len(self.no_peds) > 0:
            print('è¡€çµ±ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹é¦¬ãŒã‚ã‚Šã¾ã™')
            
    def process_categorical(self, le_horse, le_jockey, results_m):
        """ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’å‡¦ç†"""
        df = self.data_pe.copy()
        
        # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        mask_horse = df['horse_id'].isin(le_horse.classes_)
        new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()
        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])
        df['horse_id'] = le_horse.transform(df['horse_id'])
        
        mask_jockey = df['jockey_id'].isin(le_jockey.classes_)
        new_jockey_id = df['jockey_id'].mask(mask_jockey).dropna().unique()
        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])
        df['jockey_id'] = le_jockey.transform(df['jockey_id'])
        
        df['horse_id'] = df['horse_id'].astype('category')
        df['jockey_id'] = df['jockey_id'].astype('category')
        
        # ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ– - æ–°è¦ãƒ‡ãƒ¼ã‚¿ã§åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ä½œæˆ
        available_columns = df.columns.tolist()
        categorical_cols = []
        
        # å„ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã«ã¤ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
        if 'weather' in df.columns and 'weather' in results_m.columns:
            weathers = results_m['weather'].unique()
            df['weather'] = pd.Categorical(df['weather'], weathers)
            categorical_cols.append('weather')
        elif 'weather' in df.columns:
            # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®å ´åˆã¯æ—¢å­˜ã®å€¤ã‚’ä½¿ç”¨
            categorical_cols.append('weather')
            
        if 'race_type' in df.columns and 'race_type' in results_m.columns:
            race_types = results_m['race_type'].unique()
            df['race_type'] = pd.Categorical(df['race_type'], race_types)
            categorical_cols.append('race_type')
        elif 'race_type' in df.columns:
            categorical_cols.append('race_type')
            
        if 'ground_state' in df.columns and 'ground_state' in results_m.columns:
            ground_states = results_m['ground_state'].unique()
            df['ground_state'] = pd.Categorical(df['ground_state'], ground_states)
            categorical_cols.append('ground_state')
        elif 'ground_state' in df.columns:
            categorical_cols.append('ground_state')
            
        if 'æ€§' in df.columns and 'æ€§' in results_m.columns:
            sexes = results_m['æ€§'].unique()
            df['æ€§'] = pd.Categorical(df['æ€§'], sexes)
            categorical_cols.append('æ€§')
        elif 'æ€§' in df.columns:
            categorical_cols.append('æ€§')
        
        # ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–ã‚’å®Ÿè¡Œï¼ˆåˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ ã®ã¿ï¼‰
        if categorical_cols:
            df = pd.get_dummies(df, columns=categorical_cols)
        
        self.data_c = df


class ModelEvaluator:
    """æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model, return_tables=None):
        self.model = model
        self.training_columns = None  # è¨“ç·´æ™‚ã®ç‰¹å¾´é‡åˆ—ã‚’ä¿å­˜
        if return_tables is not None:
            if isinstance(return_tables, list):
                self.rt = Return.read_pickle(return_tables)
            else:
                self.rt = return_tables if isinstance(return_tables, Return) else Return(return_tables)
            self.fukusho = self.rt.fukusho
            self.tansho = self.rt.tansho
    
    def predict_proba(self, X, train=True, std=True, minmax=False):
        """3ç€ä»¥å†…ã«å…¥ã‚‹ç¢ºç‡ã‚’äºˆæ¸¬"""
        # ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        X_pred = X.copy()
        
        # é™¤å¤–ã™ã‚‹åˆ—ã®ãƒªã‚¹ãƒˆ
        exclude_columns = ['å˜å‹', 'date', 'year', 'horse_id', 'jockey_id', 'rank']
        
        # é™¤å¤–åˆ—ã‚’å‰Šé™¤
        for col in exclude_columns:
            if col in X_pred.columns:
                X_pred = X_pred.drop(col, axis=1)
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã®çµ±ä¸€ï¼ˆDateTimeå‹ã‚’é™¤å¤–ã—ã¦æ•°å€¤å‹ã®ã¿ã«ã™ã‚‹ï¼‰
        datetime_columns = []
        for col in X_pred.columns:
            if X_pred[col].dtype == 'datetime64[ns]':
                datetime_columns.append(col)
            elif X_pred[col].dtype == 'object':
                try:
                    X_pred[col] = pd.to_numeric(X_pred[col], errors='coerce').fillna(0)
                except:
                    datetime_columns.append(col)
        
        # DateTimeå‹ã‚„å‡¦ç†ã§ããªã„åˆ—ã‚’å‰Šé™¤
        if datetime_columns:
            X_pred = X_pred.drop(datetime_columns, axis=1)
        
        # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
        X_pred = X_pred.fillna(0)
        
        # è¨“ç·´æ™‚ã®ç‰¹å¾´é‡ã«åˆã‚ã›ã‚‹
        if hasattr(self, 'training_columns') and self.training_columns is not None:
            # è¨“ç·´æ™‚ã®ç‰¹å¾´é‡åˆ—ã«åˆã‚ã›ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª¿æ•´
            for col in self.training_columns:
                if col not in X_pred.columns:
                    X_pred[col] = 0  # ä¸è¶³ã™ã‚‹ç‰¹å¾´é‡ã‚’0ã§è£œå®Œ
            
            # ä½™åˆ†ãªç‰¹å¾´é‡ã‚’å‰Šé™¤ã—ã€é †åºã‚’åˆã‚ã›ã‚‹
            X_pred = X_pred[self.training_columns]
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ¢ãƒ‡ãƒ«ã®æœŸå¾…ã™ã‚‹ç‰¹å¾´é‡æ•°ã«èª¿æ•´
            if hasattr(self.model, 'n_features_in_'):
                expected_features = self.model.n_features_in_
                current_features = X_pred.shape[1]
                
                if current_features < expected_features:
                    # ä¸è¶³ã™ã‚‹ç‰¹å¾´é‡ã‚’0ã§åŸ‹ã‚ã‚‹
                    for i in range(current_features, expected_features):
                        X_pred[f'feature_{i}'] = 0
                elif current_features > expected_features:
                    # ä½™åˆ†ãªç‰¹å¾´é‡ã‚’å‰Šé™¤
                    X_pred = X_pred.iloc[:, :expected_features]
        
        # äºˆæ¸¬å®Ÿè¡Œ
        try:
            probabilities = self.model.predict_proba(X_pred.values)[:, 1]
            proba = pd.Series(probabilities, index=X.index)
        except Exception as e:
            print(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_pred.shape}")
            if hasattr(self.model, 'n_features_in_'):
                print(f"ãƒ¢ãƒ‡ãƒ«æœŸå¾…ç‰¹å¾´é‡æ•°: {self.model.n_features_in_}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ©ãƒ³ãƒ€ãƒ ãªäºˆæ¸¬å€¤ã‚’è¿”ã™
            proba = pd.Series([0.3] * len(X), index=X.index)
        
        if std:
            # ãƒ¬ãƒ¼ã‚¹å†…ã§æ¨™æº–åŒ–ã—ã¦ã€ç›¸å¯¾è©•ä¾¡ã™ã‚‹
            standard_scaler = lambda x: (x - x.mean()) / x.std(ddof=0)
            proba = proba.groupby(level=0).transform(standard_scaler)
        if minmax:
            # ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’0~1ã«ã™ã‚‹
            proba = (proba - proba.min()) / (proba.max() - proba.min())
        return proba
    
    def predict(self, X, threshold=0.5):
        """0ã‹1ã‹ã‚’äºˆæ¸¬"""
        y_pred = self.predict_proba(X)
        self.proba = y_pred
        return [0 if p<threshold else 1 for p in y_pred]
    
    def score(self, y_true, X):
        """ROC-AUCã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        return roc_auc_score(y_true, self.predict_proba(X))
    
    def feature_importance(self, X, n_display=20):
        """ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—"""
        importances = pd.DataFrame({"features": X.columns, 
                                    "importance": self.model.feature_importances_})
        return importances.sort_values("importance", ascending=False)[:n_display]
    
    def pred_table(self, X, threshold=0.5, bet_only=True):
        """äºˆæ¸¬ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ"""
        pred_table = X.copy()[['é¦¬ç•ª', 'å˜å‹']]
        pred_table['pred'] = self.predict(X, threshold)
        pred_table['score'] = self.proba
        if bet_only:
            return pred_table[pred_table['pred']==1][['é¦¬ç•ª', 'å˜å‹', 'score']]
        else:
            return pred_table[['é¦¬ç•ª', 'å˜å‹', 'score', 'pred']]
    
    def bet(self, race_id, kind, umaban, amount):
        """è³­ã‘ã®çµæœã‚’è¨ˆç®—"""
        if kind == 'fukusho' and hasattr(self, 'fukusho'):
            rt_1R = self.fukusho.loc[race_id]
            return_ = (rt_1R[['win_0', 'win_1', 'win_2']]==umaban).values * \
                rt_1R[['return_0', 'return_1', 'return_2']].values * amount/100
            return_ = np.sum(return_)
        elif kind == 'tansho' and hasattr(self, 'tansho'):
            rt_1R = self.tansho.loc[race_id]
            return_ = (rt_1R['win']==umaban) * rt_1R['return'] * amount/100
        else:
            return_ = 0
            
        if not (return_ >= 0):
            return_ = amount
        return return_
        
    def fukusho_return(self, X, threshold=0.5):
        """è¤‡å‹ã®å›åç‡ã‚’è¨ˆç®—"""
        pred_table = self.pred_table(X, threshold)
        n_bets = len(pred_table)
        
        if n_bets == 0:
            return 0, 0.0, 0, 0.0
        
        return_list = []
        for race_id, preds in pred_table.groupby(level=0):
            try:
                return_list.append(np.sum([
                    self.bet(race_id, 'fukusho', umaban, 1) for umaban in preds['é¦¬ç•ª']
                ]))
            except:
                return_list.append(0)
                
        return_rate = np.sum(return_list) / n_bets if n_bets > 0 else 0
        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets if n_bets > 0 else 0
        n_hits = np.sum([x>0 for x in return_list])
        return n_bets, return_rate, n_hits, std
    
    def tansho_return(self, X, threshold=0.5):
        """å˜å‹ã®å›åç‡ã‚’è¨ˆç®—"""
        pred_table = self.pred_table(X, threshold)
        n_bets = len(pred_table)
        
        if n_bets == 0:
            return 0, 0.0, 0, 0.0
        
        return_list = []
        for race_id, preds in pred_table.groupby(level=0):
            try:
                return_list.append(
                    np.sum([self.bet(race_id, 'tansho', umaban, 1) for umaban in preds['é¦¬ç•ª']])
                )
            except:
                return_list.append(0)
        
        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets if n_bets > 0 else 0
        n_hits = np.sum([x>0 for x in return_list])
        return_rate = np.sum(return_list) / n_bets if n_bets > 0 else 0
        return n_bets, return_rate, n_hits, std


def gain(return_func, X, n_samples=50, t_range=[0.5, 3.5]):
    """é–¾å€¤ã‚’å¤‰åŒ–ã•ã›ã¦å›åç‡ã‚’è¨ˆç®—"""
    gain_data = {}
    for i in tqdm(range(n_samples)):
        threshold = t_range[1] * i / n_samples + t_range[0] * (1-(i/n_samples))
        try:
            n_bets, return_rate, n_hits, std = return_func(X, threshold)
            if n_bets > 2:
                gain_data[threshold] = {
                    'return_rate': return_rate, 
                    'n_hits': n_hits,
                    'std': std,
                    'n_bets': n_bets
                }
        except Exception as e:
            print(f"é–¾å€¤ {threshold:.3f} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    if not gain_data:
        # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        return pd.DataFrame({
            0.5: {'return_rate': 1.0, 'n_hits': 0, 'std': 0.0, 'n_bets': 0}
        }).T
    
    return pd.DataFrame(gain_data).T

def plot_return_rate(df, label=' '):
    """æ¨™æº–åå·®ã¤ãå›åç‡ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    plt.fill_between(df.index, y1=df['return_rate']-df['std'],
                 y2=df['return_rate']+df['std'],
                 alpha=0.3)
    plt.plot(df.index, df['return_rate'], label=label)
    plt.legend()
    plt.grid(True)


class DataProcessor:
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def update_data(old_df, new_df):
        """å¤ã„ãƒ‡ãƒ¼ã‚¿ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ãƒ»æ›´æ–°"""
        filtered_df = old_df[~old_df.index.isin(new_df.index)]
        return pd.concat([filtered_df, new_df])
    
    @staticmethod
    def standard_scaler(x):
        """æ¨™æº–åŒ–é–¢æ•°ï¼ˆãƒ¬ãƒ¼ã‚¹å†…ã§ã®ç›¸å¯¾è©•ä¾¡ç”¨ï¼‰"""
        return (x - x.mean()) / x.std()


class Results(DataProcessor):
    """ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ã‚¯ãƒ©ã‚¹ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰"""
    
    def __init__(self, results):
        super(Results, self).__init__()
        self.data = results
        self.data_p = pd.DataFrame()
        self.data_h = pd.DataFrame()
        self.data_pe = pd.DataFrame()
        self.data_c = pd.DataFrame()
        
    @classmethod
    def read_pickle(cls, path_list):
        """pickleãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        df = pd.read_pickle(path_list[0])
        for path in path_list[1:]:
            df = DataProcessor.update_data(df, pd.read_pickle(path))
        return cls(df)
    
    def preprocessing(self):
        """å‰å‡¦ç†ã‚’å®Ÿè¡Œ"""
        df = self.data.copy()

        # ç€é †ã®æ•°å€¤åŒ–ã¨ãƒ©ãƒ³ã‚¯å¤‰æ›ï¼ˆ3ç€ä»¥å†…ã‚’1ã€ãã‚Œä»¥å¤–ã‚’0ï¼‰
        df['rank'] = pd.to_numeric(df['rank'], errors='coerce')
        df.dropna(subset=['rank'], inplace=True)
        df['rank'] = df['rank'].astype(int)
        df['target'] = df['rank'].map(lambda x: 1 if x <= 3 else 0)

        # æ€§é½¢ã‚’æ€§ã¨å¹´é½¢ã«åˆ†é›¢
        df["æ€§"] = df["sex_age"].map(lambda x: str(x)[0])
        df["å¹´é½¢"] = df["sex_age"].map(lambda x: str(x)[1:]).astype(int)

        # é¦¬ä½“é‡ã‚’ä½“é‡ã¨ä½“é‡å¤‰åŒ–ã«åˆ†é›¢
        df["ä½“é‡"] = df["weight_and_diff"].str.split("(", expand=True)[0]
        df["ä½“é‡å¤‰åŒ–"] = df["weight_and_diff"].str.split("(", expand=True)[1].str[:-1]
        
        df['ä½“é‡'] = pd.to_numeric(df['ä½“é‡'], errors='coerce')
        df['ä½“é‡å¤‰åŒ–'] = pd.to_numeric(df['ä½“é‡å¤‰åŒ–'], errors='coerce')

        # å˜å‹ã‚ªãƒƒã‚ºã®å¤‰æ›
        df["å˜å‹"] = pd.to_numeric(df["tansho"], errors='coerce')
        
        # è·é›¢ã®å¤‰æ›ï¼ˆ100må˜ä½ã«ï¼‰
        df["course_len"] = pd.to_numeric(df["course_len"], errors='coerce') // 100

        # æ ç•ªã€é¦¬ç•ªã®å‡¦ç†
        df["æ ç•ª"] = pd.to_numeric(df["frame_num"], errors='coerce')
        df["é¦¬ç•ª"] = pd.to_numeric(df["horse_num"], errors='coerce')
        
        # æ–¤é‡ã®å‡¦ç†
        df["æ–¤é‡"] = pd.to_numeric(df["weight"], errors='coerce')

        # å¹´ã‚’ä½¿ã£ã¦ç°¡æ˜“çš„ãªæ—¥ä»˜ã‚’ä½œæˆ
        df["year"] = pd.to_numeric(df["year"], errors='coerce').fillna(2020).astype(int)
        df["date"] = pd.to_datetime(df["year"], format='%Y')

        # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
        drop_columns = ["time", "diff", "trainer", "sex_age", "weight_and_diff", 
                       'horse_name', 'jockey', 'popularity', 'frame_num', 
                       'horse_num', 'weight', 'tansho']
        existing_drop_columns = [col for col in drop_columns if col in df.columns]
        df.drop(existing_drop_columns, axis=1, inplace=True)
        
        # é–‹å‚¬å ´æ‰€ã®æ•°å€¤åŒ–
        df['é–‹å‚¬'] = df.index.map(lambda x: str(x)[4:6] if len(str(x)) > 5 else '01')
        
        # å‡ºèµ°é ­æ•°ã®è¿½åŠ 
        df['n_horses'] = df.index.map(df.index.value_counts())

        # rankåˆ—ã‚’targetã«å¤‰æ›´
        df['rank'] = df['target']
        df.drop('target', axis=1, inplace=True)

        self.data_p = df
        
        # åŸºæœ¬çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æº–å‚™ï¼ˆhorse_idã¨jockey_idãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        if 'horse_id' in df.columns and 'jockey_id' in df.columns:
            from sklearn.preprocessing import LabelEncoder
            self.le_horse = LabelEncoder().fit(df['horse_id'])
            self.le_jockey = LabelEncoder().fit(df['jockey_id'])
        else:
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãŒä½œæˆã§ããªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ã‚’ä½œæˆ
            from sklearn.preprocessing import LabelEncoder
            self.le_horse = LabelEncoder()
            self.le_jockey = LabelEncoder()
            # é©å½“ãªãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§fitã™ã‚‹
            self.le_horse.fit(['dummy_horse'])
            self.le_jockey.fit(['dummy_jockey'])
        
        print(f"å‰å‡¦ç†å®Œäº†: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
    
    def merge_horse_results(self, hr, n_samples_list=[5, 9, 'all']):
        """é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ"""
        self.data_h = self.data_p.copy()
        for n_samples in n_samples_list:
            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)
            
        # é¦¬ã®å‡ºèµ°é–“éš”ã‚’è¿½åŠ 
        self.data_h['interval'] = (self.data_h['date'] - self.data_h['latest']).dt.days
        self.data_h.drop(['é–‹å‚¬', 'latest'], axis=1, inplace=True, errors='ignore')
        
    def merge_peds(self, peds):
        """è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ"""
        self.data_pe = self.data_h.merge(peds, left_on='horse_id', right_index=True, how='left')
        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]['horse_id'].unique()
        if len(self.no_peds) > 0:
            print('è¡€çµ±ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹é¦¬ãŒã‚ã‚Šã¾ã™: no_pedsã‚’ç¢ºèªã—ã¦ãã ã•ã„')
            
    def process_categorical(self):
        """ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®å‡¦ç†"""
        self.le_horse = LabelEncoder().fit(self.data_pe['horse_id'])
        self.le_jockey = LabelEncoder().fit(self.data_pe['jockey_id'])
        
        df = self.data_pe.copy()
        
        # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        df['horse_id'] = self.le_horse.transform(df['horse_id'])
        df['jockey_id'] = self.le_jockey.transform(df['jockey_id'])
        
        # ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›
        df['horse_id'] = df['horse_id'].astype('category')
        df['jockey_id'] = df['jockey_id'].astype('category')
        
        # ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–
        categorical_columns = ['weather', 'race_type', 'ground_state', 'æ€§']
        existing_categorical = [col for col in categorical_columns if col in df.columns]
        
        if existing_categorical:
            df = pd.get_dummies(df, columns=existing_categorical)
        
        self.data_c = df


class WebScraper:
    """netkeiba.comã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    
    @staticmethod
    def get_race_info(race_id):
        """ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã¨å‡ºé¦¬è¡¨ã‚’å–å¾—"""
        try:
            url = f'https://race.netkeiba.com/race/shutuba.html?race_id={race_id}'
            headers = {'User-Agent': random.choice(USER_AGENTS)}
            response = requests.get(url, headers=headers)
            response.encoding = "EUC-JP"
            
            if response.status_code != 200:
                return None
                
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±
            race_info = WebScraper._extract_race_info(soup)
            
            # å‡ºèµ°é¦¬æƒ…å ±
            entries = WebScraper._extract_entries(soup)
            
            return {
                'race_id': race_id,
                'race_name': race_info.get('name', ''),
                'distance': race_info.get('distance', 1600),
                'course_type': race_info.get('course_type', 'èŠ'),
                'weather': race_info.get('weather', 'æ™´'),
                'ground_state': race_info.get('ground_state', 'è‰¯'),
                'entries': entries,
                'n_horses': len(entries)
            }
            
        except Exception as e:
            print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    @staticmethod
    def _extract_race_info(soup):
        """ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ã®æŠ½å‡º"""
        race_info = {}
        
        try:
            # ãƒ¬ãƒ¼ã‚¹å
            title_elem = soup.find('h1', class_='raceTitle')
            if title_elem:
                race_info['name'] = title_elem.text.strip()
            
            # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
            race_data = soup.find('div', class_='RaceData01')
            if race_data:
                text = race_data.text
                
                # è·é›¢
                distance_match = re.search(r'(\d+)m', text)
                if distance_match:
                    race_info['distance'] = int(distance_match.group(1))
                
                # ã‚³ãƒ¼ã‚¹ç¨®åˆ¥
                if 'èŠ' in text:
                    race_info['course_type'] = 'èŠ'
                elif 'ãƒ€' in text:
                    race_info['course_type'] = 'ãƒ€ãƒ¼ãƒˆ'
                elif 'éšœ' in text:
                    race_info['course_type'] = 'éšœå®³'
                
                # å¤©å€™
                weather_patterns = ["æ›‡", "æ™´", "é›¨", "å°é›¨", "å°é›ª", "é›ª"]
                for weather in weather_patterns:
                    if weather in text:
                        race_info['weather'] = weather
                        break
                
                # é¦¬å ´çŠ¶æ…‹
                ground_patterns = ["è‰¯", "ç¨é‡", "é‡", "ä¸è‰¯"]
                for ground in ground_patterns:
                    if ground in text:
                        race_info['ground_state'] = ground
                        break
                        
        except Exception as e:
            print(f"ãƒ¬ãƒ¼ã‚¹æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return race_info
    
    @staticmethod
    def _extract_entries(soup):
        """å‡ºèµ°é¦¬æƒ…å ±ã®æŠ½å‡º"""
        entries = []
        
        try:
            # å‡ºèµ°è¡¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
            tables = pd.read_html(soup.prettify())
            if not tables:
                return entries
                
            entry_table = tables[0]
            
            # é¦¬IDã¨é¨æ‰‹IDã‚’æŠ½å‡º
            horse_ids = WebScraper._extract_ids(soup, 'HorseInfo')
            jockey_ids = WebScraper._extract_ids(soup, 'Jockey')
            
            for i, row in entry_table.iterrows():
                try:
                    entry = {
                        'waku_umaban': f"{row.get('æ ', i+1)}-{row.get('é¦¬ç•ª', i+1)}",
                        'horse_name': row.get('é¦¬å', ''),
                        'jockey_name': row.get('é¨æ‰‹', ''),
                        'sex_age': row.get('æ€§é½¢', ''),
                        'weight': row.get('æ–¤é‡', ''),
                        'horse_weight': row.get('é¦¬ä½“é‡', ''),
                        'odds': row.get('å˜å‹', ''),
                        'horse_id': horse_ids[i] if i < len(horse_ids) else '',
                        'jockey_id': jockey_ids[i] if i < len(jockey_ids) else ''
                    }
                    entries.append(entry)
                except Exception as e:
                    print(f"å‡ºèµ°é¦¬{i+1}ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
                    
        except Exception as e:
            print(f"å‡ºèµ°è¡¨æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return entries
    
    @staticmethod
    def _extract_ids(soup, class_name):
        """IDæƒ…å ±ã®æŠ½å‡º"""
        ids = []
        try:
            elements = soup.find_all("td", class_=class_name)
            for element in elements:
                link = element.find('a')
                if link and 'href' in link.attrs:
                    id_match = re.search(r'(\d+)', link['href'])
                    if id_match:
                        ids.append(id_match.group(1))
        except Exception as e:
            print(f"IDæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return ids


class DataPreprocessor:
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.label_encoders = {}
    
    def preprocess_race_data(self, race_info):
        """ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        if not race_info or not race_info.get('entries'):
            return None
        
        processed_entries = []
        
        for entry in race_info['entries']:
            processed = self._process_single_entry(entry, race_info)
            if processed:
                processed_entries.append(processed)
        
        if not processed_entries:
            return None
        
        df = pd.DataFrame(processed_entries)
        df = self._add_race_features(df, race_info)
        
        return df
    
    def _process_single_entry(self, entry, race_info):
        """å˜ä¸€ã®å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
        try:
            processed = {}
            
            # åŸºæœ¬æƒ…å ±
            processed['horse_id'] = self._convert_to_numeric(entry.get('horse_id', ''))
            processed['jockey_id'] = self._convert_to_numeric(entry.get('jockey_id', ''))
            
            # é¦¬ç•ªãƒ»æ ç•ª
            waku_umaban = entry.get('waku_umaban', '1-1')
            if '-' in waku_umaban:
                waku, umaban = waku_umaban.split('-')
                processed['æ ç•ª'] = int(waku)
                processed['é¦¬ç•ª'] = int(umaban)
            else:
                processed['æ ç•ª'] = 1
                processed['é¦¬ç•ª'] = 1
            
            # å¹´é½¢
            sex_age = entry.get('sex_age', '')
            age_match = re.search(r'(\d+)', sex_age)
            processed['å¹´é½¢'] = int(age_match.group(1)) if age_match else 4
            
            # æ–¤é‡
            weight_str = entry.get('weight', '56')
            weight_match = re.search(r'(\d+)', weight_str)
            processed['æ–¤é‡'] = int(weight_match.group(1)) if weight_match else 56
            
            # é¦¬ä½“é‡ã¨ä½“é‡å¤‰åŒ–
            horse_weight_str = entry.get('horse_weight', '480(0)')
            weight_match = re.search(r'(\d+)', horse_weight_str)
            change_match = re.search(r'\(([+-]?\d+)\)', horse_weight_str)
            
            processed['ä½“é‡'] = int(weight_match.group(1)) if weight_match else 480
            processed['ä½“é‡å¤‰åŒ–'] = int(change_match.group(1)) if change_match else 0
            
            # ã‚ªãƒƒã‚º
            odds_str = entry.get('odds', '0')
            odds_match = re.search(r'(\d+\.\d+)', odds_str)
            processed['å˜å‹'] = float(odds_match.group(1)) if odds_match else 0.0
            
            return processed
            
        except Exception as e:
            print(f"å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _add_race_features(self, df, race_info):
        """ãƒ¬ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’è¿½åŠ """
        df['course_len'] = race_info.get('distance', 1600) // 100
        df['n_horses'] = race_info.get('n_horses', 16)
        
        # ãƒ¬ãƒ¼ã‚¹ç¨®åˆ¥ï¼ˆãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
        course_type = race_info.get('course_type', 'èŠ')
        df['race_type_èŠ'] = (course_type == 'èŠ').astype(int)
        df['race_type_ãƒ€ãƒ¼ãƒˆ'] = (course_type == 'ãƒ€ãƒ¼ãƒˆ').astype(int)
        df['race_type_éšœå®³'] = (course_type == 'éšœå®³').astype(int)
        
        # å¤©å€™ãƒ»é¦¬å ´çŠ¶æ…‹
        df['weather'] = race_info.get('weather', 'æ™´')
        df['ground_state'] = race_info.get('ground_state', 'è‰¯')
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        df['é–‹å‚¬'] = 1
        df['date'] = datetime.now().strftime('%Y-%m-%d')
        df['year'] = datetime.now().year
        
        return df
    
    @staticmethod
    def _convert_to_numeric(id_str):
        """IDã‚’æ•°å€¤ã«å¤‰æ›"""
        try:
            if id_str:
                return sum(ord(c) for c in str(id_str)) % 100000
            return 0
        except:
            return 0


class ModelTrainer:
    """æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.model = None
        self.feature_columns = None
        self.label_encoders = {}
        self.best_params = None
        self.cv_scores = None
    
    def train(self, data_path='data/data/results.pickle'):
        """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        try:
            df = pd.read_pickle(data_path)
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        print(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}")
        
        # å‰å‡¦ç†
        df_processed = self._preprocess_training_data(df)
        
        if df_processed is None or len(df_processed) == 0:
            print("å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
        X, y = self._prepare_features_and_target(df_processed)
        
        if X is None or y is None:
            print("ç‰¹å¾´é‡ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        print("Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é–‹å§‹...")
        best_params = self._optimize_hyperparameters(X, y)
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        print("æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        self._train_final_model(X, y, best_params)
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
        return True
    
    def _optimize_hyperparameters(self, X, y, n_trials=100):
        """Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        
        def objective(trial):
            # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®å®šç¾©
            params = {
                'objective': 'binary',
                'metric': 'auc',
                'boosting_type': 'gbdt',
                'random_state': 42,
                'verbose': -1,
                
                # æœ€é©åŒ–å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                'num_leaves': trial.suggest_int('num_leaves', 10, 300),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
                'max_depth': trial.suggest_int('max_depth', 3, 12),
                'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),
            }
            
            # 5-fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            cv_scores = []
            
            for train_idx, val_idx in skf.split(X, y):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
                train_data = lgb.Dataset(X_train, label=y_train)
                val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
                
                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                model = lgb.train(
                    params=params,
                    train_set=train_data,
                    valid_sets=[val_data],
                    num_boost_round=1000,
                    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]
                )
                
                # äºˆæ¸¬ã¨ã‚¹ã‚³ã‚¢è¨ˆç®—
                y_pred = model.predict(X_val, num_iteration=model.best_iteration)
                score = roc_auc_score(y_val, y_pred)
                cv_scores.append(score)
            
            return np.mean(cv_scores)
        
        # Optunaæœ€é©åŒ–å®Ÿè¡Œ
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42)
        )
        
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        print(f"âœ… æœ€é©åŒ–å®Œäº†ï¼æœ€é«˜AUC: {study.best_value:.4f}")
        print(f"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study.best_params}")
        
        self.best_params = study.best_params
        return study.best_params
    
    def _train_final_model(self, X, y, best_params):
        """æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        
        # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        final_params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'random_state': 42,
            'verbose': -1,
            **best_params
        }
        
        # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
        
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        self.model = lgb.train(
            params=final_params,
            train_set=train_data,
            valid_sets=[train_data, val_data],
            valid_names=['train', 'eval'],
            num_boost_round=1000,
            callbacks=[
                lgb.early_stopping(stopping_rounds=100),
                lgb.log_evaluation(100)
            ]
        )
        
        # æ€§èƒ½è©•ä¾¡
        train_pred = self.model.predict(X_train, num_iteration=self.model.best_iteration)
        val_pred = self.model.predict(X_val, num_iteration=self.model.best_iteration)
        
        train_auc = roc_auc_score(y_train, train_pred)
        val_auc = roc_auc_score(y_val, val_pred)
        
        # ãƒã‚¤ãƒŠãƒªäºˆæ¸¬ç”¨ã®é–¾å€¤
        train_pred_binary = (train_pred > 0.5).astype(int)
        val_pred_binary = (val_pred > 0.5).astype(int)
        
        train_acc = accuracy_score(y_train, train_pred_binary)
        val_acc = accuracy_score(y_val, val_pred_binary)
        
        print(f"\nğŸ“Š æœ€çµ‚ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
        print(f"  è¨“ç·´AUC: {train_auc:.4f} | æ¤œè¨¼AUC: {val_auc:.4f}")
        print(f"  è¨“ç·´ç²¾åº¦: {train_acc:.4f} | æ¤œè¨¼ç²¾åº¦: {val_acc:.4f}")
        
        # ç‰¹å¾´é‡é‡è¦åº¦ã®è¡¨ç¤º
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': self.model.feature_importance(importance_type='gain')
        }).sort_values('importance', ascending=False)
        
        print(f"\nğŸ” é‡è¦ç‰¹å¾´é‡ãƒˆãƒƒãƒ—10:")
        for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
            print(f"  {i+1:2d}. {row['feature'][:20]:20s}: {row['importance']:8.1f}")
        
        # sklearnäº’æ›ã®ãƒ¢ãƒ‡ãƒ«ã‚‚ä½œæˆï¼ˆäºˆæ¸¬ç”¨ï¼‰
        self.sklearn_model = lgb.LGBMClassifier(**final_params)
        self.sklearn_model.fit(X_train, y_train)
        
        self.feature_columns = X.columns.tolist()
    
    def _preprocess_training_data(self, df):
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        try:
            # åŸºæœ¬çš„ãªå‰å‡¦ç†
            processed_df = df.copy()
            
            # ç€é †ã®å‡¦ç†ï¼ˆ3ç€ä»¥å†…ã‚’1ã€ãã‚Œä»¥å¤–ã‚’0ï¼‰
            if 'rank' in processed_df.columns:
                processed_df['rank'] = pd.to_numeric(processed_df['rank'], errors='coerce')
                processed_df = processed_df.dropna(subset=['rank'])
                processed_df['target'] = (processed_df['rank'] <= 3).astype(int)
            else:
                print("'rank'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            # æ€§é½¢ã®åˆ†é›¢
            if 'sex_age' in processed_df.columns:
                processed_df["æ€§"] = processed_df["sex_age"].astype(str).str[0]
                processed_df["å¹´é½¢"] = pd.to_numeric(
                    processed_df["sex_age"].astype(str).str[1:], errors='coerce'
                ).fillna(4).astype(int)
            
            # é¦¬ä½“é‡ã®åˆ†é›¢
            if 'weight_and_diff' in processed_df.columns:
                weight_split = processed_df["weight_and_diff"].str.split("(", expand=True)
                processed_df["ä½“é‡"] = pd.to_numeric(weight_split[0], errors='coerce').fillna(480)
                if weight_split.shape[1] > 1:
                    processed_df["ä½“é‡å¤‰åŒ–"] = pd.to_numeric(
                        weight_split[1].str[:-1], errors='coerce'
                    ).fillna(0)
                else:
                    processed_df["ä½“é‡å¤‰åŒ–"] = 0
            
            # æ•°å€¤å¤‰æ›
            numeric_columns = ['course_len', 'æ ç•ª', 'é¦¬ç•ª', 'æ–¤é‡', 'å˜å‹']
            for col in numeric_columns:
                if col in processed_df.columns:
                    processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')
            
            # è·é›¢ã‚’100må˜ä½ã«
            if 'course_len' in processed_df.columns:
                processed_df["course_len"] = processed_df["course_len"].fillna(1600) // 100
            
            # æ—¥ä»˜å‡¦ç†
            if 'date' in processed_df.columns:
                processed_df['date'] = pd.to_datetime(processed_df['date'], errors='coerce')
            elif 'year' in processed_df.columns:
                processed_df['year'] = pd.to_numeric(processed_df['year'], errors='coerce').fillna(2020)
                processed_df['date'] = pd.to_datetime(processed_df['year'], format='%Y')
            
            # ä¸è¦åˆ—ã®å‰Šé™¤
            drop_columns = [
                'time', 'diff', 'trainer', 'sex_age', 'weight_and_diff',
                'horse_name', 'jockey', 'popularity', 'frame_num',
                'horse_num', 'weight', 'tansho'
            ]
            existing_drop_columns = [col for col in drop_columns if col in processed_df.columns]
            processed_df = processed_df.drop(existing_drop_columns, axis=1, errors='ignore')
            
            return processed_df
            
        except Exception as e:
            print(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _prepare_features_and_target(self, df):
        """ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æº–å‚™"""
        try:
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
            if 'target' not in df.columns:
                print("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•° 'target' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None, None
                
            y = df['target']
            
            # é™¤å¤–ã™ã‚‹åˆ—
            exclude_columns = [
                'target', 'rank', 'date', 'horse_id', 'jockey_id', 'year'
            ]
            
            # ç‰¹å¾´é‡
            X = df.drop([col for col in exclude_columns if col in df.columns], axis=1)
            
            # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®å‡¦ç†
            for col in X.columns:
                if X[col].dtype == 'object':
                    if col not in self.label_encoders:
                        self.label_encoders[col] = LabelEncoder()
                        X[col] = self.label_encoders[col].fit_transform(X[col].astype(str))
                    else:
                        X[col] = self.label_encoders[col].transform(X[col].astype(str))
            
            # æ¬ æå€¤å‡¦ç†
            X = X.fillna(0)
            
            self.feature_columns = X.columns.tolist()
            
            print(f"ç‰¹å¾´é‡æ•°: {len(self.feature_columns)}")
            print(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(X)}")
            print(f"æ­£ä¾‹ç‡: {y.mean():.3f}")
            
            return X, y
            
        except Exception as e:
            print(f"ç‰¹å¾´é‡æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None
    
    
    def save_model(self, model_path='horse_racing_model.pkl'):
        """Optunaã§æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ä¿å­˜"""
        try:
            # ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            model_data = {
                'model': self.model,  # LightGBM native model
                'sklearn_model': getattr(self, 'sklearn_model', None),  # sklearnäº’æ›ãƒ¢ãƒ‡ãƒ«
                'feature_columns': self.feature_columns,
                'label_encoders': self.label_encoders,
                'best_params': getattr(self, 'best_params', None),
                'performance_metrics': getattr(self, 'performance_metrics', None),
                'cv_scores': getattr(self, 'cv_scores', None),
                'training_timestamp': datetime.now().isoformat(),
                'model_type': 'lightgbm_optuna_optimized'
            }
            
            # ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
            with open(model_path, 'wb') as f:
                pickle.dump(model_data, f)
            
            # è©³ç´°æƒ…å ±ã®JSONä¿å­˜ï¼ˆäººé–“ãŒèª­ã‚ã‚‹å½¢å¼ï¼‰
            info_path = model_path.replace('.pkl', '_info.json')
            model_info = {
                'feature_columns': self.feature_columns,
                'best_params': getattr(self, 'best_params', None),
                'performance_metrics': getattr(self, 'performance_metrics', None),
                'training_timestamp': datetime.now().isoformat(),
                'model_type': 'lightgbm_optuna_optimized',
                'feature_count': len(self.feature_columns) if self.feature_columns else 0
            }
            
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†:")
            print(f"  ä¸»ãƒ•ã‚¡ã‚¤ãƒ«: {model_path}")
            print(f"  è©³ç´°æƒ…å ±: {info_path}")
            
            if hasattr(self, 'best_params') and self.best_params:
                print(f"  æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(self.best_params)}")
            if hasattr(self, 'performance_metrics') and self.performance_metrics:
                val_auc = self.performance_metrics.get('val_auc', 'N/A')
                print(f"  æ¤œè¨¼AUC: {val_auc}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def load_model(self, model_path='horse_racing_model.pkl'):
        """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            if not Path(model_path).exists():
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
                return False
            
            print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {model_path}")
            
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒ
            self.model = model_data['model']
            self.sklearn_model = model_data.get('sklearn_model', None)
            self.feature_columns = model_data['feature_columns']
            self.label_encoders = model_data.get('label_encoders', {})
            self.best_params = model_data.get('best_params', None)
            self.performance_metrics = model_data.get('performance_metrics', None)
            self.cv_scores = model_data.get('cv_scores', None)
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
            training_time = model_data.get('training_timestamp', 'Unknown')
            model_type = model_data.get('model_type', 'Unknown')
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†:")
            print(f"  è¨“ç·´æ—¥æ™‚: {training_time}")
            print(f"  ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥: {model_type}")
            print(f"  ç‰¹å¾´é‡æ•°: {len(self.feature_columns)}")
            
            if self.best_params:
                print(f"  æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(self.best_params)}")
                
            if self.performance_metrics:
                val_auc = self.performance_metrics.get('val_auc', 'N/A')
                val_accuracy = self.performance_metrics.get('val_accuracy', 'N/A')
                print(f"  æ¤œè¨¼AUC: {val_auc}")
                print(f"  æ¤œè¨¼ç²¾åº¦: {val_accuracy}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False


class HorseRacingPredictor:
    """ç«¶é¦¬äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""

    def predict_from_manual_input(self, race_data, horses_list):
        """
        æ‰‹å‹•å…¥åŠ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹çµæœã‚’äºˆæ¸¬ã™ã‚‹ã€‚

        Args:
            race_data (dict): ãƒ¬ãƒ¼ã‚¹å…¨ä½“ã«é–¢ã™ã‚‹æƒ…å ±ï¼ˆä¾‹: course_len, race_type, dateï¼‰ã€‚
            horses_list (list): å„é¦¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã—ãŸè¾æ›¸ã®ãƒªã‚¹ãƒˆã€‚

        Returns:
            pandas.DataFrame: äºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        """
        if not self.is_trained:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return None
        
        if not hasattr(self, 'results') or self.results is None:
            print("âŒ äºˆæ¸¬ã«å¿…è¦ãªè¨“ç·´æ™‚ã®ãƒ‡ãƒ¼ã‚¿(results)ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return None

        try:
            df = pd.DataFrame(horses_list)
            for key, value in race_data.items():
                df[key] = value
            
            df['horse_id'] = df['horse_name'].apply(name_to_id)
            df['jockey_id'] = df['jockey_name'].apply(name_to_id)

            df['æ€§é½¢'] = df['æ€§'] + df['å¹´é½¢'].astype(str)
            df['é¦¬ä½“é‡(å¢—æ¸›)'] = df['ä½“é‡'].astype(str) + '(' + df['ä½“é‡å¤‰åŒ–'].astype(str) + ')'
            df.rename(columns={'æ ç•ª': 'æ ', 'horse_name': 'é¦¬å', 'jockey_name': 'é¨æ‰‹'}, inplace=True)
            
            race_id = pd.to_datetime(race_data.get('date')).strftime('%Y%m%d%H%M') + "M"
            df.index = [race_id] * len(df)

            shutuba_table = ShutubaTable(df)
            self.last_shutuba_table = shutuba_table
            shutuba_table.preprocessing()

            shutuba_table.data_h = shutuba_table.data_p.copy()
            shutuba_table.data_pe = shutuba_table.data_h.copy()

            shutuba_table.process_categorical(
                self.results.le_horse,
                self.results.le_jockey,
                self.results.data_c
            )
            
            X = shutuba_table.data_c.copy()
            X.drop(['date'], axis=1, inplace=True, errors='ignore')

            if self.model_evaluator is None:
                print("âŒ ModelEvaluatorãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                return None

            pred_scores = self.model_evaluator.predict_proba(X, minmax=True)

            results_df = pd.DataFrame({
                'é¦¬ç•ª': df['é¦¬ç•ª'],
                'é¦¬å': df['é¦¬å'],
                'score': pred_scores
            })
            results_df = results_df.sort_values('score', ascending=False).reset_index(drop=True)

            return results_df

        except Exception as e:
            import traceback
            print(f"âŒ æ‰‹å‹•å…¥åŠ›ã‹ã‚‰ã®äºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            traceback.print_exc()
            return None

    def _update_pickle_file(self, file_path, new_data_df):
        """Helper to load a pickle, update it with new data, and save it back."""
        file = Path(file_path)
        if file.exists():
            try:
                old_data_df = pd.read_pickle(file)
                updated_df = DataProcessor.update_data(old_data_df, new_data_df)
            except EOFError:
                print(f"è­¦å‘Š: {file_path} ãŒç©ºã¾ãŸã¯ç ´æã—ã¦ã„ã‚‹ãŸã‚ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ä¸Šæ›¸ãã—ã¾ã™ã€‚")
                updated_df = new_data_df
        else:
            updated_df = new_data_df
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é‡è¤‡ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰ä¿å­˜
        updated_df = updated_df[~updated_df.index.duplicated(keep='last')]
        updated_df.to_pickle(file)
        print(f"âœ… {file_path} ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

    def _convert_shutuba_to_results_format(self, shutuba_df, ranks):
        """Converts a raw shutuba dataframe to the format of results.pickle."""
        results_df = pd.DataFrame()
        results_df.index = shutuba_df.index
        
        # This function must handle dataframes from both scraping and manual input
        
        # --- Columns that need conditional logic ---
        
        # Rank
        uma_ban_col = 'é¦¬ç•ª' if 'é¦¬ç•ª' in shutuba_df.columns else 'horse_num'
        results_df['rank'] = shutuba_df[uma_ban_col].map(ranks)

        # Names
        horse_name_col = 'é¦¬å' if 'é¦¬å' in shutuba_df.columns else 'horse_name'
        jockey_name_col = 'é¨æ‰‹' if 'é¨æ‰‹' in shutuba_df.columns else 'jockey_name'
        results_df['horse_name'] = shutuba_df[horse_name_col]
        results_df['jockey'] = shutuba_df[jockey_name_col]

        # Sex and Age
        if 'æ€§é½¢' in shutuba_df.columns:
            results_df['sex_age'] = shutuba_df['æ€§é½¢']
        else:
            results_df['sex_age'] = shutuba_df['æ€§'] + shutuba_df['å¹´é½¢'].astype(str)

        # Horse Weight and Diff
        if 'é¦¬ä½“é‡(å¢—æ¸›)' in shutuba_df.columns:
            results_df['weight_and_diff'] = shutuba_df['é¦¬ä½“é‡(å¢—æ¸›)']
        else:
            results_df['weight_and_diff'] = shutuba_df['ä½“é‡'].astype(str) + '(' + shutuba_df['ä½“é‡å¤‰åŒ–'].astype(str) + ')'
            
        # Frame number
        frame_num_col = 'æ ' if 'æ ' in shutuba_df.columns else 'æ ç•ª'
        results_df['frame_num'] = shutuba_df[frame_num_col]

        # --- Columns that should be consistent ---
        
        results_df['tansho'] = shutuba_df.get('å˜å‹', 0)
        results_df['course_len'] = shutuba_df['course_len'] # Should be in 100m units
        results_df['horse_num'] = shutuba_df[uma_ban_col]
        results_df['weight'] = shutuba_df['æ–¤é‡']
        results_df['date'] = pd.to_datetime(shutuba_df['date'])
        results_df['year'] = results_df['date'].dt.year
        results_df['horse_id'] = shutuba_df['horse_id']
        results_df['jockey_id'] = shutuba_df['jockey_id']
        results_df['weather'] = shutuba_df.get('weather', 'æ™´')
        results_df['race_type'] = shutuba_df.get('race_type', 'èŠ')
        results_df['ground_state'] = shutuba_df.get('ground_state', 'è‰¯')
        
        return results_df

    def _create_horse_results_from_race(self, results_df):
        """Creates a dataframe for horse_results.pickle from a completed race."""
        hr_df = pd.DataFrame()
        
        # Map back from code to location name for 'venue'
        inv_place_dict = {v: k for k, v in PLACE_DICT.items()}
        
        hr_df['date'] = results_df['date']
        hr_df['venue'] = results_df.index.map(lambda x: str(x)[4:6]).map(inv_place_dict)
        hr_df['weather'] = results_df['weather']
        hr_df['race_num'] = results_df.index.map(lambda x: str(x)[-2:]) 
        hr_df['race_name'] = 'N/A' # Not available
        hr_df['n_horses'] = results_df.groupby(level=0)['horse_id'].transform('count')
        hr_df['frame_num'] = results_df['frame_num']
        hr_df['horse_num'] = results_df['horse_num']
        hr_df['horse_name'] = results_df['horse_name']
        hr_df['sex_age'] = results_df['sex_age']
        hr_df['weight'] = results_df['weight']
        hr_df['jockey'] = results_df['jockey']
        hr_df['rank'] = results_df['rank']
        hr_df['tansho'] = results_df['tansho']
        hr_df['popularity'] = 0 # Not available
        hr_df['time'] = '0:00.0' # Not available
        hr_df['diff'] = 0.0 # Not available
        hr_df['passing'] = '' # Not available
        hr_df['goal_time'] = 0.0 # Not available
        hr_df['prize'] = 0.0 # Not available
        hr_df['weight_and_diff'] = results_df['weight_and_diff']
        
        # Set horse_id as the index
        hr_df = hr_df.set_index(results_df['horse_id'])
        
        return hr_df

    def _create_return_table_from_data(self, race_id, return_data):
        """Creates a dataframe for return_tables.pickle from payout data."""
        rows = []
        
        # å˜å‹
        tansho = return_data.get('tansho')
        if tansho and tansho.get('win') and tansho.get('return'):
            rows.append({'0': 'å˜å‹', '1': str(tansho['win']), '2': str(tansho['return'])})
        
        # è¤‡å‹
        fukusho = return_data.get('fukusho')
        if fukusho and fukusho.get('win') and fukusho.get('return'):
            win_str = 'br'.join(map(str, fukusho['win']))
            return_str = 'br'.join(map(str, fukusho['return']))
            rows.append({'0': 'è¤‡å‹', '1': win_str, '2': return_str})
        
        if not rows:
            return pd.DataFrame()
            
        df = pd.DataFrame(rows)
        df.index = [race_id] * len(df)
        return df

    def add_results_to_training_data(self, raw_shutuba_df, ranks, return_data):
        """
        Adds the results of a completed race to the training data files.

        Args:
            raw_shutuba_df (pd.DataFrame): The DataFrame with the initial race data, 
                                          before heavy preprocessing.
            ranks (dict): A dictionary mapping horse numbers (é¦¬ç•ª) to their final rank (ç€é †).
            return_data (dict): A dictionary with payout information.
        """
        try:
            if raw_shutuba_df.empty:
                print("âŒ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
                return False
                
            race_id = raw_shutuba_df.index[0]
            
            # --- 1. Update results.pickle ---
            new_results_df = self._convert_shutuba_to_results_format(raw_shutuba_df, ranks)
            self._update_pickle_file('data/data/results.pickle', new_results_df)

            # --- 2. Update horse_results.pickle ---
            new_horse_results_df = self._create_horse_results_from_race(new_results_df)
            self._update_pickle_file('data/data/horse_results.pickle', new_horse_results_df)

            # --- 3. Update return_tables.pickle ---
            new_return_table_df = self._create_return_table_from_data(race_id, return_data)
            if not new_return_table_df.empty:
                self._update_pickle_file('data/data/return_tables.pickle', new_return_table_df)

            return True
        except Exception as e:
            import traceback
            print(f"âŒ ãƒ¬ãƒ¼ã‚¹çµæœã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            traceback.print_exc()
            return False
    
    def __init__(self):
        self.scraper = WebScraper()
        self.preprocessor = DataPreprocessor()
        self.trainer = ModelTrainer()
        self.is_trained = False
        self.last_shutuba_table = None # äºˆæ¸¬æ™‚ã®å‡ºé¦¬è¡¨ã‚’ä¸€æ™‚ä¿å­˜
        
        # è¨“ç·´æ¸ˆã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        self.results = None
        self.horse_results = None
        self.peds = None
        self.model_evaluator = None
    
    def train_model(self, data_path='data/data/results.pickle', n_trials=100):
        """Optunaã‚’ä½¿ã£ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("=== ğŸš€ Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–è¨“ç·´ ===")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        try:
            print("ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
            self.results = Results.read_pickle([data_path])
            self.results.preprocessing()
            print(f"âœ… ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿: {len(self.results.data_p)}ä»¶")
            
            # HorseResultsãƒ‡ãƒ¼ã‚¿ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            try:
                horse_results_path = data_path.replace('results', 'horse_results')
                self.horse_results = HorseResults.read_pickle([horse_results_path])
                print(f"âœ… é¦¬éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿: {len(self.horse_results.horse_results)}ä»¶")
                
                # é¦¬éå»æˆç¸¾ã‚’çµåˆ
                self.results.merge_horse_results(self.horse_results, n_samples_list=[5, 9, 'all'])
                print("âœ… é¦¬éå»æˆç¸¾çµåˆå®Œäº†")
            except:
                print("âš ï¸ é¦¬éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            
            # è¡€çµ±ãƒ‡ãƒ¼ã‚¿ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            try:
                peds_path = data_path.replace('results', 'peds')
                self.peds = Peds.read_pickle([peds_path])
                self.peds.encode()
                self.results.merge_peds(self.peds.peds_e)
                print(f"âœ… è¡€çµ±ãƒ‡ãƒ¼ã‚¿: {len(self.peds.peds_e)}ä»¶")
            except:
                print("âš ï¸ è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            
            # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°å‡¦ç†
            self.results.process_categorical()
            print(f"âœ… æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: {len(self.results.data_c)}ä»¶, ç‰¹å¾´é‡: {self.results.data_c.shape[1]}")
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        # Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        try:
            print(f"\nğŸ”§ Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–é–‹å§‹ (è©¦è¡Œå›æ•°: {n_trials})")
            
            # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢
            X = self.results.data_c.drop(['rank', 'date', 'å˜å‹'], axis=1, errors='ignore')
            y = self.results.data_c['rank']
            
            # ãƒ‡ãƒ¼ã‚¿å‹ã®çµ±ä¸€ï¼ˆDateTimeå‹ã‚’é™¤å¤–ï¼‰
            datetime_columns = []
            for col in X.columns:
                if X[col].dtype == 'datetime64[ns]':
                    datetime_columns.append(col)
                elif X[col].dtype == 'object':
                    try:
                        X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)
                    except:
                        datetime_columns.append(col)
            
            # DateTimeå‹ã‚„å‡¦ç†ã§ããªã„åˆ—ã‚’é™¤å¤–
            if datetime_columns:
                X = X.drop(datetime_columns, axis=1)
                print(f"é™¤å¤–ã—ãŸåˆ—: {datetime_columns}")
            
            # æ¬ æå€¤å‡¦ç†
            X = X.fillna(0)
            
            print(f"è¨“ç·´ç”¨ç‰¹å¾´é‡æ•°: {X.shape[1]}")
            print(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(X)}, æ­£ä¾‹ç‡: {y.mean():.3f}")
            
            # Optunaã«ã‚ˆã‚‹æœ€é©åŒ–
            best_params = self._optimize_hyperparameters(X, y, n_trials)
            
            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            print("ğŸ¯ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´...")
            final_model = self._train_final_model(X, y, best_params)
            
            # ModelTrainerã«çµæœã‚’ä¿å­˜
            self.trainer.model = final_model['lightgbm_model']
            self.trainer.sklearn_model = final_model['sklearn_model']
            self.trainer.feature_columns = X.columns.tolist()
            self.trainer.best_params = best_params
            self.trainer.performance_metrics = final_model['performance_metrics']
            
            # ModelEvaluatorã‚’ä½œæˆ
            try:
                return_path = data_path.replace('results', 'return_tables')
                self.model_evaluator = ModelEvaluator(final_model['sklearn_model'], [return_path])
                self.model_evaluator.training_columns = X.columns.tolist()
                print("âœ… ModelEvaluatorä½œæˆå®Œäº†ï¼ˆreturn_tablesä½¿ç”¨ï¼‰")
            except:
                self.model_evaluator = ModelEvaluator(final_model['sklearn_model'], None)
                self.model_evaluator.training_columns = X.columns.tolist()
                print("âœ… ModelEvaluatorä½œæˆå®Œäº†ï¼ˆreturn_tablesãªã—ï¼‰")
            
            # è¨“ç·´æ™‚ã®æ€§èƒ½æŒ‡æ¨™ã‚‚ä¿å­˜
            print("\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æŒ‡æ¨™ã‚’ä¿å­˜ä¸­...")
            analyzer = ResultsAnalyzer()
            analyzer.save_model_performance(
                final_model['performance_metrics'],
                final_model['feature_importance'],
                best_params
            )
            
            self.is_trained = True
            print("ğŸ† Optunaã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†ï¼")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _optimize_hyperparameters(self, X, y, n_trials):
        """Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        
        def objective(trial):
            # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®å®šç¾©
            params = {
                'objective': 'binary',
                'metric': 'auc',
                'boosting_type': 'gbdt',
                'random_state': 42,
                'verbose': -1,
                
                # æœ€é©åŒ–å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                'num_leaves': trial.suggest_int('num_leaves', 10, 300),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
                'max_depth': trial.suggest_int('max_depth', 3, 12),
                'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),
            }
            
            # 5-fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            cv_scores = []
            
            for train_idx, val_idx in skf.split(X, y):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
                train_data = lgb.Dataset(X_train, label=y_train)
                val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
                
                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                model = lgb.train(
                    params=params,
                    train_set=train_data,
                    valid_sets=[val_data],
                    num_boost_round=1000,
                    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]
                )
                
                # äºˆæ¸¬ã¨ã‚¹ã‚³ã‚¢è¨ˆç®—
                y_pred = model.predict(X_val, num_iteration=model.best_iteration)
                score = roc_auc_score(y_val, y_pred)
                cv_scores.append(score)
            
            return np.mean(cv_scores)
        
        # Optunaæœ€é©åŒ–å®Ÿè¡Œ
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42)
        )
        
        print("ğŸ” ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä¸­...")
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        print(f"âœ… æœ€é©åŒ–å®Œäº†ï¼æœ€é«˜AUC: {study.best_value:.4f}")
        print(f"ğŸ¯ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        for key, value in study.best_params.items():
            print(f"  {key}: {value}")
        
        return study.best_params
    
    def _train_final_model(self, X, y, best_params):
        """æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        
        # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        final_params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'random_state': 42,
            'verbose': -1,
            **best_params
        }
        
        # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
        
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆLightGBM nativeï¼‰
        lightgbm_model = lgb.train(
            params=final_params,
            train_set=train_data,
            valid_sets=[train_data, val_data],
            valid_names=['train', 'eval'],
            num_boost_round=1000,
            callbacks=[
                lgb.early_stopping(stopping_rounds=100),
                lgb.log_evaluation(100)
            ]
        )
        
        # sklearnäº’æ›ãƒ¢ãƒ‡ãƒ«ã‚‚ä½œæˆ
        sklearn_model = lgb.LGBMClassifier(**final_params)
        sklearn_model.fit(X_train, y_train)
        
        # æ€§èƒ½è©•ä¾¡
        train_pred = lightgbm_model.predict(X_train, num_iteration=lightgbm_model.best_iteration)
        val_pred = lightgbm_model.predict(X_val, num_iteration=lightgbm_model.best_iteration)
        
        train_auc = roc_auc_score(y_train, train_pred)
        val_auc = roc_auc_score(y_val, val_pred)
        
        # ãƒã‚¤ãƒŠãƒªäºˆæ¸¬ã®ãŸã‚ã®é–¾å€¤è¨­å®š
        train_pred_binary = (train_pred > 0.5).astype(int)
        val_pred_binary = (val_pred > 0.5).astype(int)
        
        train_acc = accuracy_score(y_train, train_pred_binary)
        val_acc = accuracy_score(y_val, val_pred_binary)
        train_precision = precision_score(y_train, train_pred_binary)
        val_precision = precision_score(y_val, val_pred_binary)
        train_recall = recall_score(y_train, train_pred_binary)
        val_recall = recall_score(y_val, val_pred_binary)
        train_f1 = f1_score(y_train, train_pred_binary)
        val_f1 = f1_score(y_val, val_pred_binary)
        
        print(f"\nğŸ“Š æœ€çµ‚ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
        print(f"  è¨“ç·´AUC: {train_auc:.4f} | æ¤œè¨¼AUC: {val_auc:.4f}")
        print(f"  è¨“ç·´ç²¾åº¦: {train_acc:.4f} | æ¤œè¨¼ç²¾åº¦: {val_acc:.4f}")
        print(f"  è¨“ç·´F1: {train_f1:.4f} | æ¤œè¨¼F1: {val_f1:.4f}")
        
        # ç‰¹å¾´é‡é‡è¦åº¦ã®è¡¨ç¤º
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': lightgbm_model.feature_importance(importance_type='gain')
        }).sort_values('importance', ascending=False)
        
        print(f"\nğŸ” é‡è¦ç‰¹å¾´é‡ãƒˆãƒƒãƒ—10:")
        for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
            print(f"  {i+1:2d}. {row['feature'][:20]:20s}: {row['importance']:8.1f}")
        
        # è©³ç´°ãªæ€§èƒ½æŒ‡æ¨™
        performance_metrics = {
            'train_auc': train_auc,
            'val_auc': val_auc,
            'train_accuracy': train_acc,
            'val_accuracy': val_acc,
            'train_precision': train_precision,
            'val_precision': val_precision,
            'train_recall': train_recall,
            'val_recall': val_recall,
            'train_f1': train_f1,
            'val_f1': val_f1,
            'train_size': len(X_train),
            'val_size': len(X_val),
            'n_features': len(X.columns),
            'best_iteration': lightgbm_model.best_iteration
        }
        
        return {
            'lightgbm_model': lightgbm_model,
            'sklearn_model': sklearn_model,
            'feature_importance': feature_importance,
            'performance_metrics': performance_metrics
        }
    
    def update_horse_data(self, race_id_list, date):
        """å½“æ—¥å‡ºèµ°é¦¬ã®æœ€æ–°æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆæ•™æChapter06æº–æ‹ ï¼‰"""
        print("=== äº‹å‰æº–å‚™ï¼šé¦¬ã®éå»æˆç¸¾ã¨è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–° ===")
        
        try:
            # å‡ºé¦¬è¡¨å–å¾—
            print("å‡ºé¦¬è¡¨å–å¾—ä¸­...")
            st = ShutubaTable.scrape(race_id_list, date)
            
            if len(st.data) == 0:
                print("âŒ å‡ºé¦¬è¡¨ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return False
            
            print(f"âœ… å‡ºé¦¬è¡¨å–å¾—å®Œäº†: {len(st.data)}é ­")
            
            # é¦¬ã®æœ€æ–°æˆç¸¾ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            print("é¦¬æˆç¸¾ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... (æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)")
            horse_ids = st.data['horse_id'].unique()
            today_str = date.replace('/', '')
            
            try:
                horse_results_today = HorseResults.scrape(horse_ids)
                
                # pickleãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                pickle_filename = f'horse_results_{today_str}.pickle'
                horse_results_today.to_pickle(pickle_filename)
                print(f"âœ… é¦¬æˆç¸¾ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {pickle_filename}")
                
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµåˆ
                if self.horse_results is not None:
                    original_path = 'data/data/horse_results.pickle'
                    horse_results_list = [original_path, pickle_filename]
                    existing_files = [f for f in horse_results_list if Path(f).exists()]
                    
                    if existing_files:
                        self.horse_results = HorseResults.read_pickle(existing_files)
                        print(f"âœ… HorseResultsãƒ‡ãƒ¼ã‚¿çµåˆå®Œäº†: {len(self.horse_results.horse_results)} ãƒ¬ã‚³ãƒ¼ãƒ‰")
                
            except Exception as e:
                print(f"âš ï¸ é¦¬æˆç¸¾ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                print("æ—¢å­˜ã®HorseResultsãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def predict_race_live(self, race_id, date=None, save_results=True):
        """æœ¬ç•ªå®Ÿè¡Œï¼šãƒ¬ãƒ¼ã‚¹ç›´å‰ã®äºˆæ¸¬ï¼ˆçµæœä¿å­˜æ©Ÿèƒ½ä»˜ãï¼‰"""
        if not self.is_trained:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        print("=== æœ¬ç•ªå®Ÿè¡Œï¼šãƒ¬ãƒ¼ã‚¹ç›´å‰ã®äºˆæ¸¬ ===")
        
        # æ—¥ä»˜ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•æ¨å®š
        if date is None:
            year = race_id[:4]
            month = race_id[4:6]
            day = race_id[6:8]
            date = f"{year}/{month}/{day}"
        
        print(f"äºˆæ¸¬å¯¾è±¡ãƒ¬ãƒ¼ã‚¹: {race_id} ({date})")
        
        # çµæœåˆ†æå™¨ã‚’åˆæœŸåŒ–ï¼ˆä¿å­˜ã™ã‚‹å ´åˆï¼‰
        analyzer = None
        if save_results:
            analyzer = ResultsAnalyzer()
        
        try:
            # ãƒ¬ãƒ¼ã‚¹ç›´å‰ã®å‡ºé¦¬è¡¨ã‚’å†å–å¾—
            print("æœ€æ–°ã®å‡ºé¦¬è¡¨ã‚’å–å¾—ä¸­...")
            st_final = ShutubaTable.scrape([race_id], date)
            
            # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
            if not hasattr(st_final, 'data') or st_final.data is None or len(st_final.data) == 0:
                print(f"âŒ ãƒ¬ãƒ¼ã‚¹ {race_id} ã®ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                print("åˆ©ç”¨å¯èƒ½ãªãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                return None
            
            print(f"âœ… ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(st_final.data)}é ­ã®å‡ºèµ°é¦¬")
            self.last_shutuba_table = st_final
            
            # ãƒ‡ãƒ¼ã‚¿åŠ å·¥
            print("ãƒ‡ãƒ¼ã‚¿åŠ å·¥ä¸­...")
            st_final.preprocessing()  # å‰å‡¦ç†
            
            # preprocessingã®çµæœç¢ºèª
            if not hasattr(st_final, 'data_p') or st_final.data_p is None or len(st_final.data_p) == 0:
                print(f"âŒ ãƒ¬ãƒ¼ã‚¹ {race_id} ã®å‰å‡¦ç†ã§ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã‚Šã¾ã—ãŸ")
                print("ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                return None
            
            if self.horse_results is not None:
                st_final.merge_horse_results(self.horse_results)  # é¦¬ã®éå»æˆç¸¾çµåˆ
                print("âœ… é¦¬ã®éå»æˆç¸¾çµåˆå®Œäº†")
            
            if self.peds is not None:
                st_final.merge_peds(self.peds.peds_e)  # è¡€çµ±ãƒ‡ãƒ¼ã‚¿çµåˆ
                print("âœ… è¡€çµ±ãƒ‡ãƒ¼ã‚¿çµåˆå®Œäº†")
            
            if self.results is not None:
                # Resultsã‚¯ãƒ©ã‚¹ã‹ã‚‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—
                if hasattr(self.results, 'le_horse') and hasattr(self.results, 'le_jockey'):
                    try:
                        st_final.process_categorical(self.results.le_horse, self.results.le_jockey, self.results.data_h)
                        print("âœ… ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°å‡¦ç†å®Œäº†")
                    except Exception as e:
                        print(f"âš ï¸ ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        print("âš ï¸ åŸºæœ¬çš„ãªå‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯data_peã‚’data_cã«ã‚³ãƒ”ãƒ¼
                        st_final.data_c = st_final.data_pe.copy() if hasattr(st_final, 'data_pe') else st_final.data_h.copy()
                else:
                    print("âš ï¸ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãŒãªã„å ´åˆã¯data_peã‚’data_cã«ã‚³ãƒ”ãƒ¼
                    st_final.data_c = st_final.data_pe.copy() if hasattr(st_final, 'data_pe') else st_final.data_h.copy()
            else:
                print("âš ï¸ Resultsãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                # ResultsãŒãªã„å ´åˆã¯data_peã‚’data_cã«ã‚³ãƒ”ãƒ¼
                st_final.data_c = st_final.data_pe.copy() if hasattr(st_final, 'data_pe') else st_final.data_h.copy()
            
            print("âœ… ãƒ‡ãƒ¼ã‚¿åŠ å·¥å®Œäº†")
            print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: data_c ã®çŠ¶æ…‹")
            print(f"  data_c exists: {hasattr(st_final, 'data_c')}")
            if hasattr(st_final, 'data_c'):
                print(f"  data_c shape: {st_final.data_c.shape}")
                print(f"  data_c columns: {list(st_final.data_c.columns) if hasattr(st_final.data_c, 'columns') else 'None'}")
            else:
                print("  data_c ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
                # data_cãŒä½œæˆã•ã‚Œã¦ã„ãªã„å ´åˆã€data_peã‚’ä½¿ç”¨
                if hasattr(st_final, 'data_pe') and not st_final.data_pe.empty:
                    print("  data_pe ã‚’ data_c ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™")
                    st_final.data_c = st_final.data_pe.copy()
                elif hasattr(st_final, 'data_h') and not st_final.data_h.empty:
                    print("  data_h ã‚’ data_c ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™")
                    st_final.data_c = st_final.data_h.copy()
                elif hasattr(st_final, 'data_p') and not st_final.data_p.empty:
                    print("  data_p ã‚’ data_c ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™")
                    st_final.data_c = st_final.data_p.copy()
                else:
                    print("  åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    return None
            
            print(f"ç‰¹å¾´é‡æ•°: {st_final.data_c.shape[1] if hasattr(st_final, 'data_c') and hasattr(st_final.data_c, 'shape') else 0}")
            
            # äºˆæ¸¬å®Ÿè¡Œ
            print("äºˆæ¸¬ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­...")
            prediction_data = st_final.data_c.drop(['date'], axis=1, errors='ignore')
            
            # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã®ãƒã‚§ãƒƒã‚¯
            if len(prediction_data) == 0:
                print("âŒ äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼ˆãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰")
                return None
            
            # evaluatorã¾ãŸã¯model_evaluatorã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬
            if hasattr(self, 'evaluator') and self.evaluator is not None:
                scores = self.evaluator.predict_proba(prediction_data, train=False)
            elif hasattr(self, 'model_evaluator') and self.model_evaluator is not None:
                scores = self.model_evaluator.predict_proba(prediction_data, train=False)
            else:
                print("âŒ äºˆæ¸¬ã«å¿…è¦ãªè©•ä¾¡å™¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            # çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æ•´ç†
            if 'é¦¬ç•ª' in st_final.data_c.columns:
                pred = st_final.data_c[['é¦¬ç•ª']].copy()
            else:
                print("âŒ é¦¬ç•ªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            pred['score'] = scores
            pred['horse_name'] = st_final.data.get('é¦¬å', 'ä¸æ˜')
            pred['jockey'] = st_final.data.get('é¨æ‰‹', 'ä¸æ˜')
            pred['weight'] = st_final.data.get('é¦¬ä½“é‡', 'ä¸æ˜')
            pred['odds'] = st_final.data.get('å˜å‹ã‚ªãƒƒã‚º', 'ä¸æ˜')
            
            # ã‚¹ã‚³ã‚¢ã§é™é †ã‚½ãƒ¼ãƒˆ
            result = pred.loc[race_id].sort_values('score', ascending=False)
            
            # çµæœè¡¨ç¤º
            self._display_live_results(result, race_id)
            
            # çµæœä¿å­˜
            if save_results and analyzer is not None:
                print("\nğŸ’¾ äºˆæ¸¬çµæœã‚’ä¿å­˜ä¸­...")
                
                # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚‚å«ã‚ã¦ä¿å­˜
                race_info = {
                    'race_id': race_id,
                    'date': date,
                    'venue': race_id[4:6],
                    'race_num': race_id[-2:],
                    'n_horses': len(result),
                    'prediction_timestamp': datetime.now().isoformat()
                }
                
                pred_path = analyzer.save_prediction_results(race_id, result, race_info)
                
                # äºˆæ¸¬çµæœã®å¯è¦–åŒ–ã‚‚ä½œæˆ
                self._create_prediction_visualization(result, race_id, analyzer)
                
                print(f"âœ… äºˆæ¸¬çµæœä¿å­˜å®Œäº†: {pred_path}")
            
            return result
            
        except Exception as e:
            print(f"âŒ äºˆæ¸¬å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _create_prediction_visualization(self, result, race_id, analyzer):
        """äºˆæ¸¬çµæœã®å¯è¦–åŒ–ã‚’ä½œæˆ"""
        
        plt.figure(figsize=(15, 10))
        
        # 1. Prediction Score Distribution
        plt.subplot(2, 3, 1)
        plt.hist(result['score'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')
        plt.axvline(result['score'].mean(), color='red', linestyle='--', label=f'Mean: {result["score"].mean():.3f}')
        plt.xlabel('Prediction Score')
        plt.ylabel('Number of Horses')
        plt.title('Prediction Score Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. Top 10 Horses Prediction Scores
        plt.subplot(2, 3, 2)
        top10 = result.head(10)
        bars = plt.bar(range(len(top10)), top10['score'], color='lightgreen', alpha=0.8)
        plt.xlabel('Rank')
        plt.ylabel('Prediction Score')
        plt.title('Top 10 Horses Prediction Scores')
        plt.xticks(range(len(top10)), [f"{i+1}" for i in range(len(top10))])
        
        # Show horse numbers on bars
        for i, (bar, (_, row)) in enumerate(zip(bars, top10.iterrows())):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f"#{row['é¦¬ç•ª']:.0f}", ha='center', va='bottom', fontsize=9)
        
        plt.grid(True, alpha=0.3)
        
        # 3. Horse Number vs Prediction Score Scatter Plot
        plt.subplot(2, 3, 3)
        plt.scatter(result['é¦¬ç•ª'], result['score'], alpha=0.7, s=60, c=result['score'], 
                   cmap='viridis', edgecolors='black', linewidth=0.5)
        plt.colorbar(label='Prediction Score')
        plt.xlabel('Horse Number')
        plt.ylabel('Prediction Score')
        plt.title('Horse Number vs Prediction Score')
        plt.grid(True, alpha=0.3)
        
        # 4. äºˆæ¸¬ä¿¡é ¼åº¦åˆ†æ
        plt.subplot(2, 3, 4)
        high_conf = (result['score'] > 0.6).sum()
        mid_conf = ((result['score'] > 0.4) & (result['score'] <= 0.6)).sum()
        low_conf = (result['score'] <= 0.4).sum()
        
        categories = ['High Confidence\n(>0.6)', 'Mid Confidence\n(0.4-0.6)', 'Low Confidence\n(â‰¤0.4)']
        counts = [high_conf, mid_conf, low_conf]
        colors = ['red', 'orange', 'gray']
        
        plt.pie(counts, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)
        plt.title('Prediction Confidence Distribution')
        
        # 5. Horse Number Details (Top 5)
        plt.subplot(2, 3, 5)
        top5 = result.head(5)
        y_pos = range(len(top5))
        bars = plt.barh(y_pos, top5['score'], color='gold', alpha=0.8)
        plt.yticks(y_pos, [f"#{row['é¦¬ç•ª']:.0f}" for _, row in top5.iterrows()])
        plt.xlabel('Prediction Score')
        plt.title('Top 5 Horses Prediction Scores')
        
        # Show values on bars
        for i, (bar, (_, row)) in enumerate(zip(bars, top5.iterrows())):
            plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                    f"{row['score']:.3f}", ha='left', va='center', fontsize=10)
        
        plt.grid(True, alpha=0.3)
        
        # 6. Recommendation Matrix
        plt.subplot(2, 3, 6)
        
        # Calculate recommendation levels
        recommendation_matrix = []
        score_thresholds = [0.7, 0.5, 0.3]
        labels = ['Strong Buy', 'Buy', 'Caution', 'Pass']
        
        for i, threshold in enumerate(score_thresholds):
            if i == 0:
                count = (result['score'] >= threshold).sum()
            else:
                count = ((result['score'] >= threshold) & (result['score'] < score_thresholds[i-1])).sum()
            recommendation_matrix.append(count)
        
        # Last category (Pass)
        recommendation_matrix.append((result['score'] < score_thresholds[-1]).sum())
        
        plt.bar(labels, recommendation_matrix, color=['darkgreen', 'green', 'orange', 'red'], alpha=0.7)
        plt.ylabel('Number of Horses')
        plt.title('Investment Recommendation Distribution')
        plt.xticks(rotation=45)
        
        for i, count in enumerate(recommendation_matrix):
            if count > 0:
                plt.text(i, count + 0.1, str(count), ha='center', va='bottom', fontweight='bold')
        
        plt.grid(True, alpha=0.3)
        
        plt.suptitle(f'Horse Racing AI Prediction Analysis - Race {race_id}', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        # ä¿å­˜
        viz_path = analyzer.output_dir / f'prediction_analysis_{race_id}_{analyzer.timestamp}.png'
        plt.savefig(viz_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ğŸ“Š äºˆæ¸¬åˆ†æã‚°ãƒ©ãƒ•: {viz_path}")
    
    def _display_live_results(self, results, race_id):
        """ãƒ©ã‚¤ãƒ–äºˆæ¸¬çµæœã®è¡¨ç¤º"""
        print("\nğŸ† === äºˆæ¸¬çµæœ ===")
        print(f"ãƒ¬ãƒ¼ã‚¹ID: {race_id}")
        print("é †ä½  é¦¬ç•ª  ã‚¹ã‚³ã‚¢    é¦¬å           é¨æ‰‹         é¦¬ä½“é‡")
        print("-" * 60)
        
        for i, (idx, row) in enumerate(results.head(10).iterrows(), 1):
            print(f"{i:2d}ä½  {row['é¦¬ç•ª']:2d}ç•ª  {row['score']:.4f}  {str(row['horse_name'])[:10]:10s}  {str(row['jockey'])[:8]:8s}  {str(row['weight'])}")
        
        # æŠ•è³‡åˆ¤æ–­ã®ææ¡ˆ
        print("\nğŸ’° === æŠ•è³‡åˆ¤æ–­ ===")
        top_score = results.iloc[0]['score']
        if top_score > 0.5:
            print("ğŸ”¥ é«˜ç¢ºç‡äºˆæ¸¬ï¼ç©æ¥µçš„æŠ•è³‡ã‚’æ¨å¥¨")
        elif top_score > 0.4:
            print("ğŸ“ˆ ä¸­ç¨‹åº¦ç¢ºç‡ã€‚æ…é‡ãªæŠ•è³‡ã‚’æ¤œè¨")
        else:
            print("âš ï¸ ä½ç¢ºç‡äºˆæ¸¬ã€‚æŠ•è³‡ã¯è¦‹é€ã‚Šæ¨å¥¨")
        
        print(f"æœ€é«˜ã‚¹ã‚³ã‚¢: {top_score:.4f}")
        print(f"æ¨å¥¨é¦¬ç•ª: {results.iloc[0]['é¦¬ç•ª']}ç•ª")
    
    def predict_race(self, race_id, threshold=0.3):
        """ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬ã‚’å®Ÿè¡Œï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if not self.is_trained or self.trainer.model is None:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        print(f"ğŸ‡ ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬é–‹å§‹: {race_id}")
        
        # ãƒ¬ãƒ¼ã‚¹æƒ…å ±å–å¾—
        race_info = self.scraper.get_race_info(race_id)
        if not race_info:
            print("âŒ ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        print(f"ğŸ“‹ ãƒ¬ãƒ¼ã‚¹: {race_info['race_name']}")
        print(f"ğŸ ã‚³ãƒ¼ã‚¹: {race_info['distance']}m {race_info['course_type']}")
        print(f"ğŸ å‡ºèµ°é ­æ•°: {race_info['n_horses']}")
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        processed_data = self.preprocessor.preprocess_race_data(race_info)
        if processed_data is None:
            print("âŒ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        # ç‰¹å¾´é‡ã®æº–å‚™
        feature_data = self._prepare_prediction_features(processed_data)
        if feature_data is None:
            print("âŒ ç‰¹å¾´é‡ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        # äºˆæ¸¬å®Ÿè¡Œ
        try:
            predictions = self.trainer.model.predict_proba(feature_data)[:, 1]
        except Exception as e:
            print(f"âŒ äºˆæ¸¬å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None
        
        # çµæœã®æ•´ç†
        results = self._format_results(race_info, processed_data, predictions, threshold)
        
        # çµæœè¡¨ç¤º
        self._display_results(results, race_info)
        
        return results
    
    def simulate_returns(self, test_data=None, threshold_range=[0.5, 3.5], n_samples=50, save_results=True):
        """å›åç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆçµæœä¿å­˜æ©Ÿèƒ½ä»˜ãï¼‰"""
        # evaluatorã¾ãŸã¯model_evaluatorã®å­˜åœ¨ã‚’ç¢ºèª
        has_evaluator = (
            (hasattr(self, 'evaluator') and self.evaluator is not None) or
            (hasattr(self, 'model_evaluator') and self.model_evaluator is not None)
        )
        
        if not self.is_trained or not has_evaluator:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ModelEvaluatorãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print(f"   å­¦ç¿’æ¸ˆã¿: {self.is_trained}")
            print(f"   è©•ä¾¡å™¨: {has_evaluator}")
            return None
        
        if test_data is None:
            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨
            if self.results is None:
                print("âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return None
            test_data = self.results.data_c.sample(min(1000, len(self.results.data_c)))
        
        print("ğŸ“Š å›åç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œä¸­...")
        
        # çµæœåˆ†æå™¨ã‚’åˆæœŸåŒ–
        if save_results:
            analyzer = ResultsAnalyzer()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æƒ…å ±
        test_info = {
            'n_samples': len(test_data),
            'positive_rate': test_data['rank'].mean(),
            'threshold_range': threshold_range,
            'n_threshold_samples': n_samples
        }
        
        # ä½¿ç”¨ã™ã‚‹è©•ä¾¡å™¨ã‚’æ±ºå®š
        evaluator = None
        if hasattr(self, 'evaluator') and self.evaluator is not None:
            evaluator = self.evaluator
        elif hasattr(self, 'model_evaluator') and self.model_evaluator is not None:
            evaluator = self.model_evaluator
        
        if evaluator is None:
            print("âŒ è©•ä¾¡å™¨ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return None
        
        # return_tablesã®ç¢ºèª
        if not hasattr(evaluator, 'fukusho_return') or not hasattr(evaluator, 'tansho_return'):
            print("âš ï¸ return_tablesãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ä»£æ›¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™...")
            
            # ä»£æ›¿çš„ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆreturn_tablesãªã—ã®å ´åˆï¼‰
            try:
                return self._simulate_returns_alternative(test_data, threshold_range, n_samples, save_results)
            except Exception as e:
                print(f"âŒ ä»£æ›¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
                return None
        
        # è¤‡å‹å›åç‡
        print("è¤‡å‹å›åç‡ã‚’è¨ˆç®—ä¸­...")
        fukusho_gain = gain(evaluator.fukusho_return, test_data, 
                           n_samples=n_samples, t_range=threshold_range)
        
        # å˜å‹å›åç‡
        print("å˜å‹å›åç‡ã‚’è¨ˆç®—ä¸­...")
        tansho_gain = gain(evaluator.tansho_return, test_data, 
                          n_samples=n_samples, t_range=threshold_range)
        
        # åŸºæœ¬ãƒ—ãƒ­ãƒƒãƒˆã®è¡¨ç¤º
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plot_return_rate(fukusho_gain, label='Place')
        plt.title('Place Return Rate Simulation')
        plt.xlabel('Threshold')
        plt.ylabel('Return Rate')
        
        plt.subplot(1, 2, 2)
        plot_return_rate(tansho_gain, label='Win')
        plt.title('Win Return Rate Simulation')
        plt.xlabel('Threshold')
        plt.ylabel('Return Rate')
        
        plt.tight_layout()
        plt.show()
        
        # çµæœã®ä¿å­˜ã¨è©³ç´°åˆ†æ
        if save_results:
            print("\nğŸ’¾ çµæœä¿å­˜ãƒ»åˆ†æä¸­...")
            
            # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®ä¿å­˜
            json_path, csv_path = analyzer.save_simulation_results(
                fukusho_gain, tansho_gain, test_info
            )
            
            # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ä¿å­˜
            if hasattr(self.trainer, 'best_params'):
                performance_metrics = {
                    'best_params': self.trainer.best_params,
                    'feature_count': len(self.trainer.feature_columns) if self.trainer.feature_columns else 0,
                    'test_data_size': len(test_data),
                    'simulation_timestamp': datetime.now().isoformat()
                }
                
                # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
                feature_importance = None
                if hasattr(self.trainer, 'model') and hasattr(self.trainer.model, 'feature_importance'):
                    try:
                        feature_importance = pd.DataFrame({
                            'feature': self.trainer.feature_columns,
                            'importance': self.trainer.model.feature_importance(importance_type='gain')
                        }).sort_values('importance', ascending=False)
                    except:
                        pass
                
                analyzer.save_model_performance(
                    performance_metrics, feature_importance, self.trainer.best_params
                )
            
            # åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
            analyzer.create_comprehensive_report()
            
            print(f"\nğŸ‰ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼çµæœã¯ '{analyzer.output_dir}' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        
        # çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
        print("\nğŸ“ˆ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚µãƒãƒªãƒ¼:")
        self._display_simulation_summary(fukusho_gain, tansho_gain)
        
        return {
            'fukusho_gain': fukusho_gain,
            'tansho_gain': tansho_gain,
            'test_info': test_info,
            'analyzer': analyzer if save_results else None
        }
    
    def _simulate_returns_alternative(self, test_data, threshold_range, n_samples, save_results):
        """return_tablesãŒãªã„å ´åˆã®ä»£æ›¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        print("ğŸ”„ ä»£æ›¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œä¸­...")
        
        # ä½¿ç”¨ã™ã‚‹è©•ä¾¡å™¨ã‚’æ±ºå®š
        evaluator = self.evaluator if hasattr(self, 'evaluator') and self.evaluator else self.model_evaluator
        
        # äºˆæ¸¬ç¢ºç‡ã‚’è¨ˆç®—
        X_test = test_data.drop(['rank', 'date', 'å˜å‹'], axis=1, errors='ignore')
        y_test = test_data['rank']
        
        # ç‰¹å¾´é‡ã‚’è¨“ç·´æ™‚ã¨åˆã‚ã›ã‚‹
        if hasattr(evaluator, 'training_columns'):
            aligned_X = []
            for col in evaluator.training_columns:
                if col in X_test.columns:
                    aligned_X.append(X_test[col])
                else:
                    aligned_X.append(pd.Series([0] * len(X_test), index=X_test.index))
            X_test_aligned = pd.concat(aligned_X, axis=1)
        else:
            X_test_aligned = X_test
        
        # äºˆæ¸¬å®Ÿè¡Œ
        try:
            pred_proba = evaluator.predict_proba(X_test_aligned, train=False)
        except:
            # ã‚ˆã‚ŠåŸºæœ¬çš„ãªäºˆæ¸¬
            if hasattr(evaluator, 'model'):
                pred_proba = evaluator.model.predict_proba(X_test_aligned.values)[:, 1]
            else:
                print("âŒ äºˆæ¸¬å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
        
        # é–¾å€¤ç¯„å›²ã§ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        thresholds = np.linspace(threshold_range[0], threshold_range[1], n_samples)
        
        # ä»®æƒ³çš„ãªå›åç‡ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        fukusho_data = []
        tansho_data = []
        
        for threshold in thresholds:
            # é–¾å€¤ä»¥ä¸Šã®äºˆæ¸¬ã‚’é¸æŠ
            selected = pred_proba >= threshold
            
            if selected.sum() > 0:
                # çš„ä¸­ç‡ã®è¨ˆç®—ï¼ˆ1ç€ = ãƒ©ãƒ³ã‚¯1ï¼‰
                hit_rate = (y_test[selected] == 1).mean()
                
                # ä»®æƒ³çš„ãªå›åç‡ï¼ˆçš„ä¸­ç‡ã«åŸºã¥ãï¼‰
                # è¤‡å‹ã¯3ç€ä»¥å†…ãªã®ã§çš„ä¸­ç‡ã‚’èª¿æ•´
                fukusho_hit_rate = (y_test[selected] <= 3).mean()
                tansho_hit_rate = hit_rate
                
                # å¹³å‡ã‚ªãƒƒã‚ºã‚’ä»®å®šï¼ˆå®Ÿéš›ã®ã‚ªãƒƒã‚ºãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰
                avg_fukusho_odds = 1.5  # è¤‡å‹å¹³å‡ã‚ªãƒƒã‚º
                avg_tansho_odds = 8.0   # å˜å‹å¹³å‡ã‚ªãƒƒã‚º
                
                fukusho_return = fukusho_hit_rate * avg_fukusho_odds
                tansho_return = tansho_hit_rate * avg_tansho_odds
                
                fukusho_data.append({
                    'threshold': threshold,
                    'return_rate': fukusho_return,
                    'n_bets': selected.sum(),
                    'n_hits': (y_test[selected] <= 3).sum()
                })
                
                tansho_data.append({
                    'threshold': threshold,
                    'return_rate': tansho_return,
                    'n_bets': selected.sum(),
                    'n_hits': (y_test[selected] == 1).sum()
                })
            else:
                fukusho_data.append({
                    'threshold': threshold,
                    'return_rate': 0.0,
                    'n_bets': 0,
                    'n_hits': 0
                })
                
                tansho_data.append({
                    'threshold': threshold,
                    'return_rate': 0.0,
                    'n_bets': 0,
                    'n_hits': 0
                })
        
        # DataFrameã«å¤‰æ›
        fukusho_gain = pd.DataFrame(fukusho_data).set_index('threshold')
        tansho_gain = pd.DataFrame(tansho_data).set_index('threshold')
        
        # ãƒ†ã‚¹ãƒˆæƒ…å ±
        test_info = {
            'n_samples': len(test_data),
            'positive_rate': (y_test == 1).mean(),
            'threshold_range': threshold_range,
            'n_threshold_samples': n_samples,
            'simulation_type': 'alternative'
        }
        
        print("âœ… ä»£æ›¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")
        print(f"ğŸ“Š æœ€é«˜è¤‡å‹å›åç‡: {fukusho_gain['return_rate'].max():.3f}")
        print(f"ğŸ“Š æœ€é«˜å˜å‹å›åç‡: {tansho_gain['return_rate'].max():.3f}")
        
        # çµæœã®ä¿å­˜
        analyzer = None
        if save_results:
            analyzer = ResultsAnalyzer()
            analyzer.save_simulation_results(fukusho_gain, tansho_gain, test_info)
        
        return {
            'fukusho_gain': fukusho_gain,
            'tansho_gain': tansho_gain,
            'test_info': test_info,
            'analyzer': analyzer
        }
    
    def _display_simulation_summary(self, fukusho_gain, tansho_gain):
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        
        # æœ€é«˜å›åç‡ã®é–¾å€¤ã‚’æ¤œç´¢
        best_fukusho_idx = fukusho_gain['return_rate'].idxmax()
        best_tansho_idx = tansho_gain['return_rate'].idxmax()
        
        best_fukusho = fukusho_gain.loc[best_fukusho_idx]
        best_tansho = tansho_gain.loc[best_tansho_idx]
        
        print(f"""
ğŸ† è¤‡å‹æˆ¦ç•¥ï¼ˆæœ€é«˜å›åç‡ï¼‰
  é–¾å€¤: {best_fukusho_idx:.3f}
  å›åç‡: {best_fukusho['return_rate']:.3f} ({(best_fukusho['return_rate']-1)*100:+.1f}%)
  æŠ•æ³¨æ•°: {best_fukusho['n_bets']:.0f}å›
  çš„ä¸­æ•°: {best_fukusho['n_hits']:.0f}å›
  çš„ä¸­ç‡: {best_fukusho['n_hits']/best_fukusho['n_bets']*100:.1f}%

ğŸ¯ å˜å‹æˆ¦ç•¥ï¼ˆæœ€é«˜å›åç‡ï¼‰
  é–¾å€¤: {best_tansho_idx:.3f}
  å›åç‡: {best_tansho['return_rate']:.3f} ({(best_tansho['return_rate']-1)*100:+.1f}%)
  æŠ•æ³¨æ•°: {best_tansho['n_bets']:.0f}å›
  çš„ä¸­æ•°: {best_tansho['n_hits']:.0f}å›
  çš„ä¸­ç‡: {best_tansho['n_hits']/best_tansho['n_bets']*100:.1f}%

ğŸ’¡ æ¨å¥¨æˆ¦ç•¥:
  {'ğŸ”¥ è¤‡å‹ãƒ»å˜å‹ã¨ã‚‚ã«æœ‰åŠ¹ï¼' if min(best_fukusho['return_rate'], best_tansho['return_rate']) > 1.05 
   else 'ğŸ“ˆ è¤‡å‹æ¨å¥¨' if best_fukusho['return_rate'] > best_tansho['return_rate']
   else 'ğŸ¯ å˜å‹æ¨å¥¨' if best_tansho['return_rate'] > best_fukusho['return_rate']
   else 'âš ï¸ æ…é‡ãªæŠ•è³‡ã‚’æ¨å¥¨'}
""")
    
    def _prepare_prediction_features(self, processed_data):
        """äºˆæ¸¬ç”¨ç‰¹å¾´é‡ã®æº–å‚™"""
        try:
            # å¿…è¦ãªç‰¹å¾´é‡ã‚’æƒãˆã‚‹
            feature_data = pd.DataFrame()
            
            for col in self.trainer.feature_columns:
                if col in processed_data.columns:
                    feature_data[col] = processed_data[col]
                else:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§è£œå®Œ
                    if col.startswith('peds_'):
                        feature_data[col] = 0
                    elif col in ['rank_5R', 'rank_9R', 'rank_allR']:
                        feature_data[col] = 8.0  # å¹³å‡çš„ãªç€é †
                    elif col in ['prize_5R', 'prize_9R', 'prize_allR']:
                        feature_data[col] = 0.0
                    else:
                        feature_data[col] = 0
            
            # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ•°å€¤å‹ã«å¤‰æ›
            for col in feature_data.columns:
                if feature_data[col].dtype == 'object':
                    feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce').fillna(0)
            
            return feature_data.values.astype('float32')
            
        except Exception as e:
            print(f"ç‰¹å¾´é‡æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _format_results(self, race_info, processed_data, predictions, threshold):
        """çµæœã®æ•´ç†"""
        results = pd.DataFrame()
        
        entries = race_info['entries']
        
        results['é¦¬ç•ª'] = processed_data['é¦¬ç•ª']
        results['horse_name'] = [entry.get('horse_name', '') for entry in entries]
        results['jockey_name'] = [entry.get('jockey_name', '') for entry in entries]
        results['prediction_proba'] = predictions
        results['prediction'] = (predictions > threshold).astype(int)
        results['recommendation'] = results['prediction'].map({0: 'âœ—', 1: 'â—'})
        results['å˜å‹'] = processed_data['å˜å‹']
        
        return results.sort_values('prediction_proba', ascending=False)
    
    def _display_results(self, results, race_info):
        """çµæœã®è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ¯ äºˆæ¸¬çµæœ")
        print("=" * 60)
        
        print(f"ãƒ¬ãƒ¼ã‚¹: {race_info['race_name']}")
        print(f"ã‚³ãƒ¼ã‚¹: {race_info['distance']}m {race_info['course_type']}")
        
        print("\nå…¨é¦¬ã®äºˆæ¸¬çµæœ:")
        print("é¦¬ç•ª  é¦¬å              é¨æ‰‹         ç¢ºç‡    æ¨å¥¨  ã‚ªãƒƒã‚º")
        print("-" * 60)
        
        for _, horse in results.head(10).iterrows():
            odds_str = f"{horse['å˜å‹']:.1f}" if horse['å˜å‹'] > 0 else "---"
            print(f"{horse['é¦¬ç•ª']:2.0f}ç•ª  {horse['horse_name'][:12]:12s}  "
                  f"{horse['jockey_name'][:8]:8s}  {horse['prediction_proba']:.3f}  "
                  f"{horse['recommendation']:2s}  {odds_str}")
        
        # æ¨å¥¨é¦¬
        recommended = results[results['prediction'] == 1]
        if not recommended.empty:
            print(f"\nğŸ¯ æ¨å¥¨é¦¬ ({len(recommended)}é ­):")
            for _, horse in recommended.iterrows():
                odds_info = f" (ã‚ªãƒƒã‚º: {horse['å˜å‹']:.1f})" if horse['å˜å‹'] > 0 else ""
                print(f"ğŸ† {horse['é¦¬ç•ª']:2.0f}ç•ª {horse['horse_name']} "
                      f"- ç¢ºç‡: {horse['prediction_proba']:.3f}{odds_info}")
        else:
            print("\nâš ï¸ æ¨å¥¨é–¾å€¤ã‚’æº€ãŸã™é¦¬ãŒã„ã¾ã›ã‚“")
            top_horse = results.iloc[0]
            print(f"æœ€æœ‰åŠ›: {top_horse['é¦¬ç•ª']:2.0f}ç•ª {top_horse['horse_name']} "
                  f"- ç¢ºç‡: {top_horse['prediction_proba']:.3f}")
        
        # çµ±è¨ˆæƒ…å ±
        avg_prob = results['prediction_proba'].mean()
        max_prob = results['prediction_proba'].max()
        print(f"\nğŸ“Š çµ±è¨ˆ:")
        print(f"å¹³å‡ç¢ºç‡: {avg_prob:.3f}")
        print(f"æœ€é«˜ç¢ºç‡: {max_prob:.3f}")
        print(f"æ¨å¥¨é ­æ•°: {len(recommended)}/{len(results)}")
    
    def save_model(self, model_path='horse_racing_model.pkl', include_results=True):
        """
        ãƒ¢ãƒ‡ãƒ«ã¨ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®çŠ¶æ…‹ã‚’ä¿å­˜
        
        Parameters:
        -----------
        model_path : str
            ä¿å­˜å…ˆã®ãƒ‘ã‚¹
        include_results : bool
            çµæœãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã¦ä¿å­˜ã™ã‚‹ã‹
        
        Returns:
        --------
        bool
            ä¿å­˜ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            # åŸºæœ¬ãƒ‘ã‚¹ã‹ã‚‰æ‹¡å¼µå­ã‚’é™¤å»
            base_path = model_path.replace('.pkl', '')
            
            # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            model_success = self.trainer.save_model(f"{base_path}_model.pkl")
            if not model_success:
                print("âŒ ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ä¿å­˜
            system_state = {
                'is_trained': self.is_trained,
                'data_loaded': hasattr(self, 'processed_data') and self.processed_data is not None,
                'evaluator_trained': hasattr(self, 'evaluator') and self.evaluator is not None,
                'created_at': datetime.now().isoformat(),
                'version': '1.0'
            }
            
            # ResultsAnalyzerã®çŠ¶æ…‹ã‚‚ä¿å­˜
            if include_results and hasattr(self, 'results_analyzer'):
                results_state = {
                    'has_results': True,
                    'last_analysis_time': getattr(self.results_analyzer, 'last_analysis_time', None)
                }
                system_state['results_analyzer'] = results_state
            
            # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
            if hasattr(self, 'processed_data') and self.processed_data is not None:
                with open(f"{base_path}_processed_data.pkl", 'wb') as f:
                    pickle.dump(self.processed_data, f)
                print(f"âœ… å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {base_path}_processed_data.pkl")
            
            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ä¿å­˜
            with open(f"{base_path}_system_state.json", 'w', encoding='utf-8') as f:
                json.dump(system_state, f, indent=2, ensure_ascii=False)
            
            # æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
            info_content = f"""# Horse Racing AI Prediction System - ä¿å­˜æƒ…å ±

## ä¿å­˜æ—¥æ™‚
{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
- å­¦ç¿’æ¸ˆã¿: {'ã¯ã„' if self.is_trained else 'ã„ã„ãˆ'}
- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ¸ˆã¿: {'ã¯ã„' if hasattr(self, 'processed_data') and self.processed_data is not None else 'ã„ã„ãˆ'}
- è©•ä¾¡å™¨æº–å‚™æ¸ˆã¿: {'ã¯ã„' if hasattr(self, 'evaluator') and self.evaluator is not None else 'ã„ã„ãˆ'}

## ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«
- ãƒ¢ãƒ‡ãƒ«: {base_path}_model.pkl
- å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿: {base_path}_processed_data.pkl  
- ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹: {base_path}_system_state.json
- ã“ã®æƒ…å ±: {base_path}_info.md

## ä½¿ç”¨æ–¹æ³•
```python
# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
predictor = HorseRacingPredictor()
success = predictor.load_model('{model_path}')
if success:
    # äºˆæ¸¬ã‚’å®Ÿè¡Œ
    results = predictor.predict_race(race_id)
```
"""
            
            with open(f"{base_path}_info.md", 'w', encoding='utf-8') as f:
                f.write(info_content)
            
            print(f"\nâœ… ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ")
            print(f"ğŸ“ ä¿å­˜å ´æ‰€: {base_path}_*")
            print(f"ğŸ“Š ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 4å€‹")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
            return False
    
    def load_model(self, model_path='horse_racing_model.pkl'):
        """
        ãƒ¢ãƒ‡ãƒ«ã¨ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿
        
        Parameters:
        -----------
        model_path : str
            èª­ã¿è¾¼ã¿å…ƒã®ãƒ‘ã‚¹
        
        Returns:
        --------
        bool
            èª­ã¿è¾¼ã¿ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å‡¦ç†
            if model_path.endswith('_model.pkl'):
                # æ—¢ã«_model.pklã§çµ‚ã‚ã£ã¦ã„ã‚‹å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                model_file_path = model_path
                base_path = model_path.replace('_model.pkl', '')
            elif os.path.exists(model_path):
                # ç›´æ¥æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼ˆä¾‹ï¼štrained_model_optuna.pklï¼‰
                model_file_path = model_path
                base_path = model_path.replace('.pkl', '')
            else:
                # åŸºæœ¬ãƒ‘ã‚¹ã‹ã‚‰_model.pklã‚’ç”Ÿæˆ
                base_path = model_path.replace('.pkl', '')
                model_file_path = f"{base_path}_model.pkl"
            
            # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
            model_success = self.trainer.load_model(model_file_path)
            if not model_success:
                print("âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # äºˆæ¸¬ã«å¿…è¦ãªåŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            print("ğŸ“Š äºˆæ¸¬ã«å¿…è¦ãªåŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            try:
                # horse_results ã®èª­ã¿è¾¼ã¿
                if os.path.exists('data/data/horse_results.pickle'):
                    with open('data/data/horse_results.pickle', 'rb') as f:
                        horse_results_data = pickle.load(f)
                    self.horse_results = HorseResults(horse_results_data)  # ãƒ‡ãƒ¼ã‚¿ã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™
                    print("âœ… é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
                else:
                    print("âš ï¸ é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    self.horse_results = None
                
                # peds ã®èª­ã¿è¾¼ã¿
                if os.path.exists('data/data/peds.pickle'):
                    with open('data/data/peds.pickle', 'rb') as f:
                        peds_data = pickle.load(f)
                    self.peds = Peds(peds_data)  # ãƒ‡ãƒ¼ã‚¿ã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™
                    self.peds.encode()  # è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆpreprocessingã§ã¯ãªãencodeï¼‰
                    print("âœ… è¡€çµ±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
                else:
                    print("âš ï¸ è¡€çµ±ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    self.peds = None
                
                # results ã®èª­ã¿è¾¼ã¿
                if os.path.exists('data/data/results.pickle'):
                    with open('data/data/results.pickle', 'rb') as f:
                        results_data = pickle.load(f)
                    self.results = Results(results_data)  # ãƒ‡ãƒ¼ã‚¿ã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™
                    self.results.preprocessing()  # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™
                    print("âœ… ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
                else:
                    print("âš ï¸ ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    self.results = None
                    
            except Exception as data_error:
                print(f"âš ï¸ åŸºæœ¬ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {data_error}")
                print("âš ï¸ äºˆæ¸¬ç²¾åº¦ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            
            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®èª­ã¿è¾¼ã¿
            system_state_path = f"{base_path}_system_state.json"
            if os.path.exists(system_state_path):
                with open(system_state_path, 'r', encoding='utf-8') as f:
                    system_state = json.load(f)
                
                print(f"ğŸ“… ä¿å­˜æ—¥æ™‚: {system_state.get('created_at', 'Unknown')}")
                print(f"ğŸ“Š ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {system_state.get('version', 'Unknown')}")
                
                # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®å¾©å…ƒ
                self.is_trained = system_state.get('is_trained', False)
                
                # ResultsAnalyzerã®çŠ¶æ…‹å¾©å…ƒ
                if 'results_analyzer' in system_state:
                    if not hasattr(self, 'results_analyzer'):
                        self.results_analyzer = ResultsAnalyzer()
                    print("âœ… ResultsAnalyzer ã®çŠ¶æ…‹ã‚’å¾©å…ƒ")
            
            # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            processed_data_path = f"{base_path}_processed_data.pkl"
            if os.path.exists(processed_data_path):
                with open(processed_data_path, 'rb') as f:
                    self.processed_data = pickle.load(f)
                print("âœ… å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ")
            
            # ModelEvaluatorã®å†åˆæœŸåŒ–
            try:
                if hasattr(self, 'processed_data') and self.processed_data is not None:
                    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯å®Œå…¨ãªModelEvaluatorã‚’ä½œæˆ
                    self.evaluator = ModelEvaluator(
                        self.trainer.model,
                        self.trainer.sklearn_model,
                        self.processed_data['X_train'],
                        self.processed_data['y_train'],
                        self.trainer.feature_columns,
                        self.trainer.label_encoders
                    )
                    print("âœ… ModelEvaluator ã‚’å®Œå…¨ã«å†åˆæœŸåŒ–")
                    
                elif self.trainer.model is not None and self.trainer.sklearn_model is not None:
                    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯åŸºæœ¬çš„ãªModelEvaluatorã‚’ä½œæˆ
                    print("âš ï¸ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚åŸºæœ¬çš„ãªModelEvaluatorã‚’ä½œæˆ...")
                    
                    try:
                        # return_tablesã®ãƒ‘ã‚¹ã‚’æ¨å®š
                        return_path = 'data/data/return_tables.pickle'
                        if Path(return_path).exists():
                            self.evaluator = ModelEvaluator(self.trainer.sklearn_model, [return_path])
                            self.evaluator.training_columns = self.trainer.feature_columns
                            print("âœ… ModelEvaluator ã‚’åŸºæœ¬ãƒ¢ãƒ¼ãƒ‰ã§ä½œæˆï¼ˆreturn_tablesä½¿ç”¨ï¼‰")
                        else:
                            # return_tablesã‚‚ãªã„å ´åˆ
                            self.evaluator = ModelEvaluator(self.trainer.sklearn_model, None)
                            self.evaluator.training_columns = self.trainer.feature_columns
                            print("âœ… ModelEvaluator ã‚’æœ€å°ãƒ¢ãƒ¼ãƒ‰ã§ä½œæˆ")
                        
                        # äºˆæ¸¬ã«å¿…è¦ãªåŸºæœ¬å±æ€§ã‚’è¨­å®š
                        if not hasattr(self.evaluator, 'model_evaluator'):
                            self.evaluator.model_evaluator = self.trainer.sklearn_model
                        
                    except Exception as eval_error:
                        print(f"âš ï¸ ModelEvaluatorä½œæˆã‚¨ãƒ©ãƒ¼: {eval_error}")
                        print("ğŸ”§ ä»£æ›¿ã®ModelEvaluatorã‚’ä½œæˆã—ã¾ã™...")
                        
                        # æœ€å°é™ã®ModelEvaluatorã‚’æ‰‹å‹•ã§ä½œæˆ
                        class MinimalEvaluator:
                            def __init__(self, model, feature_columns):
                                self.model = model
                                self.training_columns = feature_columns
                                self.feature_columns = feature_columns
                                
                            def predict_proba(self, X, train=True):
                                # ç‰¹å¾´é‡ã®é †åºã‚’åˆã‚ã›ã‚‹
                                if hasattr(X, 'columns'):
                                    # DataFrameã®å ´åˆ
                                    aligned_X = self._align_features(X)
                                else:
                                    # numpy arrayã®å ´åˆ
                                    aligned_X = X
                                
                                return self.model.predict_proba(aligned_X)[:, 1]
                            
                            def _align_features(self, X):
                                # ç‰¹å¾´é‡ã®é †åºã‚’è¨“ç·´æ™‚ã¨åˆã‚ã›ã‚‹
                                aligned_data = []
                                for col in self.training_columns:
                                    if col in X.columns:
                                        aligned_data.append(X[col])
                                    else:
                                        # æ¬ æã—ã¦ã„ã‚‹ç‰¹å¾´é‡ã¯0ã§è£œå®Œ
                                        aligned_data.append(pd.Series([0] * len(X), index=X.index))
                                
                                return pd.concat(aligned_data, axis=1).values
                        
                        self.evaluator = MinimalEvaluator(self.trainer.sklearn_model, self.trainer.feature_columns)
                        print("âœ… æœ€å°é™ã®ModelEvaluatorã‚’ä½œæˆ")
                
                else:
                    print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒä¸å®Œå…¨ã§ã™")
                    self.evaluator = None
                    
            except Exception as e:
                print(f"âŒ ModelEvaluatoråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                self.evaluator = None
            
            # model_evaluatorå±æ€§ã‚‚è¨­å®šï¼ˆäºˆæ¸¬æ™‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰
            if hasattr(self, 'evaluator') and self.evaluator is not None:
                self.model_evaluator = self.evaluator
            
            print(f"\nğŸ‰ ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")
            print(f"ğŸ“Š çŠ¶æ…‹: å­¦ç¿’æ¸ˆã¿={self.is_trained}")
            print(f"ğŸ”§ è©•ä¾¡å™¨: {'åˆ©ç”¨å¯èƒ½' if hasattr(self, 'evaluator') and self.evaluator is not None else 'æœªè¨­å®š'}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
            return False

    def calculate_prediction_accuracy(self, race_id, predictions):
        """äºˆæ¸¬çµæœã¨å®Ÿéš›ã®çµæœã‚’æ¯”è¼ƒã—ã¦æ­£è§£ç‡ã‚’è¨ˆç®—"""
        try:
            print("å®Ÿéš›ã®ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—ä¸­...")
            
            # ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—
            race_results = self._get_actual_race_results(race_id)
            if race_results is None:
                print("âŒ å®Ÿéš›ã®ãƒ¬ãƒ¼ã‚¹çµæœãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return None
            
            # äºˆæ¸¬ã¨å®Ÿéš›ã®çµæœã‚’æ¯”è¼ƒ
            accuracy = self._compare_predictions_with_results(predictions, race_results)
            return accuracy
            
        except Exception as e:
            print(f"âŒ æ­£è§£ç‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _get_actual_race_results(self, race_id):
        """ãƒ¬ãƒ¼ã‚¹IDã‹ã‚‰å®Ÿéš›ã®çµæœã‚’å–å¾—"""
        try:
            import time
            import random
            
            # netkeibaã‹ã‚‰çµæœã‚’å–å¾—
            url = f'https://race.netkeiba.com/race/result.html?race_id={race_id}'
            headers = {'User-Agent': random.choice(USER_AGENTS)}
            
            time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            html = requests.get(url, headers=headers)
            html.encoding = "EUC-JP"
            
            if html.status_code != 200:
                return None
            
            # çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è§£æ
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html.text, "html.parser")
            
            # çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
            try:
                result_tables = pd.read_html(html.text)
                if len(result_tables) == 0:
                    return None
                
                result_df = result_tables[0]
                
                # åˆ—æ•°ã«å¿œã˜ã¦åˆ—åã‚’å‹•çš„ã«è¨­å®š
                n_cols = len(result_df.columns)
                if n_cols >= 13:
                    # æ¨™æº–çš„ãªçµæœãƒ†ãƒ¼ãƒ–ãƒ«
                    result_df.columns = ['ç€é †', 'æ ç•ª', 'é¦¬ç•ª', 'é¦¬å', 'æ€§é½¢', 'æ–¤é‡', 'ã‚¸ãƒ§ãƒƒã‚­ãƒ¼', 
                                        'ã‚¿ã‚¤ãƒ ', 'ç€å·®', 'å˜å‹', 'äººæ°—', 'é¦¬ä½“é‡', 'èª¿æ•™å¸«']
                elif n_cols >= 11:
                    # ç°¡ç•¥ç‰ˆãƒ†ãƒ¼ãƒ–ãƒ«
                    result_df.columns = ['ç€é †', 'æ ç•ª', 'é¦¬ç•ª', 'é¦¬å', 'æ€§é½¢', 'æ–¤é‡', 'ã‚¸ãƒ§ãƒƒã‚­ãƒ¼', 
                                        'ã‚¿ã‚¤ãƒ ', 'ç€å·®', 'å˜å‹', 'äººæ°—']
                elif n_cols >= 8:
                    # ã‚ˆã‚ŠçŸ­ã„ãƒ†ãƒ¼ãƒ–ãƒ«
                    result_df.columns = ['ç€é †', 'æ ç•ª', 'é¦¬ç•ª', 'é¦¬å', 'æ€§é½¢', 'æ–¤é‡', 'ã‚¸ãƒ§ãƒƒã‚­ãƒ¼', 'ã‚¿ã‚¤ãƒ ']
                else:
                    # æœ€å°é™ã®ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆç€é †ã€é¦¬ç•ªã ã‘ã§ã‚‚å–å¾—ã‚’è©¦è¡Œï¼‰
                    if n_cols >= 3:
                        result_df.columns = ['ç€é †', 'æ ç•ª', 'é¦¬ç•ª'] + [f'col_{i}' for i in range(3, n_cols)]
                    else:
                        print(f"âš ï¸ åˆ—æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {n_cols}")
                        return None
                
                # ç€é †ã¨é¦¬ç•ªã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆå¿…é ˆï¼‰
                result_df['ç€é †'] = pd.to_numeric(result_df['ç€é †'], errors='coerce')
                result_df['é¦¬ç•ª'] = pd.to_numeric(result_df['é¦¬ç•ª'], errors='coerce')
                
                # äººæ°—ãŒã‚ã‚Œã°æ•°å€¤ã«å¤‰æ›
                if 'äººæ°—' in result_df.columns:
                    result_df['äººæ°—'] = pd.to_numeric(result_df['äººæ°—'], errors='coerce')
                
                # ç„¡åŠ¹ãªè¡Œã‚’å‰Šé™¤
                result_df = result_df.dropna(subset=['ç€é †', 'é¦¬ç•ª'])
                
                if len(result_df) == 0:
                    print("âš ï¸ æœ‰åŠ¹ãªãƒ¬ãƒ¼ã‚¹çµæœãŒã‚ã‚Šã¾ã›ã‚“")
                    return None
                
                print(f"âœ… ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—: {len(result_df)}é ­")
                return result_df
                
            except Exception as table_error:
                print(f"âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {table_error}")
                # HTMLã‹ã‚‰ç›´æ¥æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ä»£æ›¿æ‰‹æ®µã‚’è©¦è¡Œ
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html.text, "html.parser")
                    
                    # çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ‰‹å‹•ã§è§£æ
                    result_table = soup.find('table', {'class': 'race_table_01'})
                    if result_table:
                        rows = result_table.find_all('tr')[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        
                        data = []
                        for row in rows:
                            cells = row.find_all(['td', 'th'])
                            if len(cells) >= 3:
                                rank = cells[0].get_text(strip=True)
                                horse_num = cells[2].get_text(strip=True)
                                
                                try:
                                    rank = int(rank)
                                    horse_num = int(horse_num)
                                    data.append({'ç€é †': rank, 'é¦¬ç•ª': horse_num})
                                except:
                                    continue
                        
                        if data:
                            result_df = pd.DataFrame(data)
                            print(f"âœ… ä»£æ›¿æ–¹æ³•ã§ãƒ¬ãƒ¼ã‚¹çµæœã‚’å–å¾—: {len(result_df)}é ­")
                            return result_df
                    
                    return None
                    
                except Exception as alt_error:
                    print(f"âš ï¸ ä»£æ›¿è§£æã‚‚å¤±æ•—: {alt_error}")
                    return None
            
        except Exception as e:
            print(f"âŒ ãƒ¬ãƒ¼ã‚¹çµæœå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _compare_predictions_with_results(self, predictions, race_results):
        """äºˆæ¸¬ã¨å®Ÿéš›ã®çµæœã‚’æ¯”è¼ƒ"""
        try:
            if predictions is None or race_results is None or len(race_results) == 0:
                return None
            
            # äºˆæ¸¬ã¯é¦¬ç•ªé †ã§ä¸¦ã‚“ã§ã„ã‚‹ã¨ä»®å®š
            # race_resultsã¯ç€é †é †ãªã®ã§ã€é¦¬ç•ªã§ä¸¦ã³æ›¿ãˆ
            race_results = race_results.sort_values('é¦¬ç•ª').reset_index(drop=True)
            
            # 1ç€ã®é¦¬ç•ªã‚’å–å¾—
            winner_data = race_results[race_results['ç€é †'] == 1]
            if len(winner_data) == 0:
                print("âš ï¸ 1ç€ã®æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
                
            winner_horse_num = winner_data['é¦¬ç•ª'].iloc[0]
            
            # äºˆæ¸¬ã§æœ€ã‚‚é«˜ã„ã‚¹ã‚³ã‚¢ã‚’æŒã¤é¦¬ç•ª
            if isinstance(predictions, pd.DataFrame):
                if 'ã‚¹ã‚³ã‚¢' in predictions.columns:
                    predicted_winner_idx = predictions['ã‚¹ã‚³ã‚¢'].idxmax()
                    predicted_winner = predictions.loc[predicted_winner_idx, 'é¦¬ç•ª'] if 'é¦¬ç•ª' in predictions.columns else predicted_winner_idx + 1
                else:
                    predicted_winner = predictions.idxmax() + 1
            else:
                predicted_winner = np.argmax(predictions) + 1
            
            # 1ç€äºˆæ¸¬ãŒçš„ä¸­ã—ãŸã‹ã©ã†ã‹
            win_accuracy = 1.0 if predicted_winner == winner_horse_num else 0.0
            
            # ä¸Šä½3é ­ã®äºˆæ¸¬ç²¾åº¦ã‚‚è¨ˆç®—
            top3_actual = race_results[race_results['ç€é †'] <= 3]['é¦¬ç•ª'].tolist()
            
            if isinstance(predictions, pd.DataFrame):
                if 'ã‚¹ã‚³ã‚¢' in predictions.columns:
                    top3_predicted_df = predictions.nlargest(3, 'ã‚¹ã‚³ã‚¢')
                    top3_predicted = top3_predicted_df['é¦¬ç•ª'].tolist() if 'é¦¬ç•ª' in top3_predicted_df.columns else (top3_predicted_df.index + 1).tolist()
                else:
                    top3_predicted = predictions.nlargest(3).index.tolist()
                    top3_predicted = [x + 1 for x in top3_predicted]
            else:
                top3_predicted_idx = np.argsort(predictions)[-3:][::-1]
                top3_predicted = [x + 1 for x in top3_predicted_idx]
            
            # ä¸Šä½3é ­ã§ã®çš„ä¸­æ•°
            top3_hits = len(set(top3_actual) & set(top3_predicted))
            top3_accuracy = top3_hits / 3.0
            
            print(f"ğŸ† Actual 1st place: Horse #{winner_horse_num}")
            print(f"ğŸ¯ Predicted 1st place: Horse #{predicted_winner}")
            print(f"ğŸ¥‡ 1st place hit: {'â—‹' if win_accuracy == 1.0 else 'Ã—'}")
            print(f"ğŸ¥‰ Top 3 accuracy: {top3_accuracy:.2%} ({top3_hits}/3)")
            print(f"ğŸ” Top 3 actual: {top3_actual}")
            print(f"ğŸ” Top 3 predicted: {top3_predicted}")
            
            return {
                'win_accuracy': win_accuracy,
                'top3_accuracy': top3_accuracy,
                'actual_winner': winner_horse_num,
                'predicted_winner': predicted_winner,
                'top3_actual': top3_actual,
                'top3_predicted': top3_predicted
            }
            
        except Exception as e:
            print(f"âŒ Prediction comparison error: {e}")
            return None
    
    def validate_multiple_races(self, race_id_list):
        """è¤‡æ•°ãƒ¬ãƒ¼ã‚¹ã§ã®äºˆæ¸¬ç²¾åº¦ã‚’æ¤œè¨¼"""
        print(f"\n=== è¤‡æ•°ãƒ¬ãƒ¼ã‚¹æ¤œè¨¼é–‹å§‹ ({len(race_id_list)}ãƒ¬ãƒ¼ã‚¹) ===")
        
        results = {}
        total_win_accuracy = 0
        total_top3_accuracy = 0
        successful_predictions = 0
        
        for i, race_id in enumerate(race_id_list, 1):
            print(f"\n--- {i}/{len(race_id_list)}: {race_id} ---")
            
            try:
                # äºˆæ¸¬å®Ÿè¡Œ
                predictions = self.predict_race_live(race_id)
                
                if predictions is not None:
                    # æ­£è§£ç‡è¨ˆç®—
                    accuracy = self.calculate_prediction_accuracy(race_id, predictions)
                    
                    if accuracy is not None:
                        results[race_id] = accuracy
                        total_win_accuracy += accuracy['win_accuracy']
                        total_top3_accuracy += accuracy['top3_accuracy']
                        successful_predictions += 1
                        
                        print(f"âœ… {race_id}: 1ç€çš„ä¸­ç‡ {accuracy['win_accuracy']:.0%}")
                    else:
                        print(f"âŒ {race_id}: æ­£è§£ç‡è¨ˆç®—å¤±æ•—")
                else:
                    print(f"âŒ {race_id}: äºˆæ¸¬å¤±æ•—")
                    
            except Exception as e:
                print(f"âŒ {race_id}: ã‚¨ãƒ©ãƒ¼ - {e}")
            
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ã®å¾…æ©Ÿ
            if i < len(race_id_list):
                time.sleep(2)
        
        # å…¨ä½“ã®çµæœã‚µãƒãƒªãƒ¼
        if successful_predictions > 0:
            avg_win_accuracy = total_win_accuracy / successful_predictions
            avg_top3_accuracy = total_top3_accuracy / successful_predictions
            
            print(f"\nğŸ¯ === æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼ ===")
            print(f"æ¤œè¨¼ãƒ¬ãƒ¼ã‚¹æ•°: {successful_predictions}/{len(race_id_list)}")
            print(f"å¹³å‡1ç€çš„ä¸­ç‡: {avg_win_accuracy:.2%}")
            print(f"å¹³å‡ä¸Šä½3é ­çš„ä¸­ç‡: {avg_top3_accuracy:.2%}")
            
            return {
                'detailed_results': results,
                'avg_win_accuracy': avg_win_accuracy,
                'avg_top3_accuracy': avg_top3_accuracy,
                'successful_predictions': successful_predictions,
                'total_races': len(race_id_list)
            }
        else:
            print("âŒ æˆåŠŸã—ãŸäºˆæ¸¬ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None


def predict_specific_race(race_id, date=None):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¬ãƒ¼ã‚¹IDã§äºˆæ¸¬ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°ï¼ˆæ•™ææº–æ‹ ï¼‰
    
    Parameters:
    -----------
    race_id : str
        netkeiba.comã®ãƒ¬ãƒ¼ã‚¹ID (ä¾‹: "202105021211")
    date : str
        ãƒ¬ãƒ¼ã‚¹æ—¥ä»˜ï¼ˆä¾‹: '2021/05/30'ï¼‰ã€‚Noneã®å ´åˆã¯è‡ªå‹•æ¨å®š
    
    Returns:
    --------
    pandas.DataFrame or None
        äºˆæ¸¬çµæœã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    if 'predictor' not in globals():
        print("âŒ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«setup_prediction_system()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return None
    
    return predictor.predict_race_live(race_id, date)


def setup_prediction_system(data_path='data/data/results.pickle', n_trials=100, use_optuna=True):
    """äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆOptunaæœ€é©åŒ–å¯¾å¿œï¼‰"""
    print("ğŸ ç«¶é¦¬AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—")
    print("=" * 50)
    
    global predictor
    predictor = HorseRacingPredictor()
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    if use_optuna:
        print(f"ğŸš€ Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã§äº‹å‰æº–å‚™ã‚’é–‹å§‹ã—ã¾ã™... (è©¦è¡Œå›æ•°: {n_trials})")
        if not predictor.train_model(data_path, n_trials=n_trials):
            print("âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
    else:
        print("å¾“æ¥ã®æ‰‹æ³•ã§äº‹å‰æº–å‚™ã‚’é–‹å§‹ã—ã¾ã™...")
        # å¾“æ¥ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if not predictor.trainer.train(data_path):
            print("âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        predictor.is_trained = True
    
    print("\nâœ… äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†ï¼")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("1. å½“æ—¥ã®å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿æ›´æ–°: predictor.update_horse_data(race_id_list, date)")
    print("2. ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬å®Ÿè¡Œ: predict_specific_race('202105021211', '2021/05/30')")
    print("3. å›åç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: predictor.simulate_returns()")
    
    return predictor


def demo_daily_prediction(n_trials=50, use_optuna=True):
    """å½“æ—¥äºˆæ¸¬ã®ãƒ‡ãƒ¢å®Ÿè¡Œï¼ˆOptunaæœ€é©åŒ–å¯¾å¿œï¼‰"""
    print("ğŸ‡ ç«¶é¦¬äºˆæ¸¬ - å½“æ—¥äºˆæ¸¬ãƒ‡ãƒ¢ï¼ˆOptunaæœ€é©åŒ–ï¼‰")
    print("=" * 50)
    
    # ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    predictor = setup_prediction_system(n_trials=n_trials, use_optuna=use_optuna)
    if predictor is None:
        return
    
    # ãƒ‡ãƒ¢ç”¨è¨­å®šï¼ˆæ•™æã¨åŒã˜æ—¥ä»˜ãƒ»ãƒ¬ãƒ¼ã‚¹ï¼‰


def list_saved_models(directory='.'):
    """
    æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¸€è¦§è¡¨ç¤º
    
    Parameters:
    -----------
    directory : str
        æ¤œç´¢ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    
    Returns:
    --------
    list
        è¦‹ã¤ã‹ã£ãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    import glob
    import os
    from datetime import datetime
    
    print(f"ğŸ“ {directory} å†…ã®ä¿å­˜ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢ä¸­...")
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    model_files = glob.glob(os.path.join(directory, "*_model.pkl"))
    
    if not model_files:
        print("âŒ ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return []
    
    models_info = []
    
    for model_file in model_files:
        base_name = model_file.replace('_model.pkl', '')
        info_file = f"{base_name}_info.md"
        state_file = f"{base_name}_system_state.json"
        
        model_info = {
            'base_name': os.path.basename(base_name),
            'model_file': model_file,
            'size': os.path.getsize(model_file),
            'modified': datetime.fromtimestamp(os.path.getmtime(model_file)),
            'has_info': os.path.exists(info_file),
            'has_state': os.path.exists(state_file)
        }
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
        if os.path.exists(state_file):
            try:
                with open(state_file, 'r', encoding='utf-8') as f:
                    state_data = json.load(f)
                model_info['created_at'] = state_data.get('created_at')
                model_info['version'] = state_data.get('version')
                model_info['is_trained'] = state_data.get('is_trained')
            except:
                pass
        
        models_info.append(model_info)
    
    # æ›´æ–°æ—¥æ™‚é †ã§ã‚½ãƒ¼ãƒˆ
    models_info.sort(key=lambda x: x['modified'], reverse=True)
    
    print(f"\nğŸ“Š è¦‹ã¤ã‹ã£ãŸãƒ¢ãƒ‡ãƒ«: {len(models_info)}å€‹")
    print("-" * 80)
    
    for i, info in enumerate(models_info, 1):
        print(f"{i}. {info['base_name']}")
        print(f"   ğŸ“… æ›´æ–°: {info['modified'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   ğŸ“¦ ã‚µã‚¤ã‚º: {info['size']:,} bytes")
        if 'created_at' in info:
            created = info['created_at'].split('T')[0] if 'T' in info['created_at'] else info['created_at']
            print(f"   ğŸ—ï¸  ä½œæˆ: {created}")
        print(f"   âœ… å­¦ç¿’æ¸ˆã¿: {'ã¯ã„' if info.get('is_trained', False) else 'ã„ã„ãˆ'}")
        print(f"   ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«å®Œæ•´æ€§: {'å®Œå…¨' if info['has_info'] and info['has_state'] else 'ä¸å®Œå…¨'}")
        print()
    
    return models_info


def compare_saved_models(model_paths):
    """
    è¤‡æ•°ã®ä¿å­˜ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¯”è¼ƒ
    
    Parameters:
    -----------
    model_paths : list
        æ¯”è¼ƒã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
    """
    print("ğŸ” ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ")
    print("=" * 60)
    
    comparison_data = []
    
    for model_path in model_paths:
        base_path = model_path.replace('.pkl', '').replace('_model.pkl', '')
        model_file = f"{base_path}_model.pkl"
        
        if not os.path.exists(model_file):
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_file}")
            continue
        
        try:
            # ModelTrainerã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
            temp_trainer = ModelTrainer()
            success = temp_trainer.load_model(model_file)
            
            if success and hasattr(temp_trainer, 'performance_metrics'):
                model_data = {
                    'name': os.path.basename(base_path),
                    'file': model_file,
                    'accuracy': temp_trainer.performance_metrics.get('accuracy', 'N/A'),
                    'precision': temp_trainer.performance_metrics.get('precision', 'N/A'),
                    'recall': temp_trainer.performance_metrics.get('recall', 'N/A'),
                    'f1_score': temp_trainer.performance_metrics.get('f1_score', 'N/A'),
                    'roc_auc': temp_trainer.performance_metrics.get('roc_auc', 'N/A'),
                    'best_params': temp_trainer.best_params if hasattr(temp_trainer, 'best_params') else 'N/A'
                }
                comparison_data.append(model_data)
                
        except Exception as e:
            print(f"âŒ {model_file} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    if not comparison_data:
        print("âŒ æ¯”è¼ƒå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # æ¯”è¼ƒçµæœã‚’è¡¨ç¤º
    print(f"ğŸ“Š {len(comparison_data)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ:")
    print("-" * 60)
    
    for i, data in enumerate(comparison_data, 1):
        print(f"{i}. {data['name']}")
        print(f"   ğŸ¯ ç²¾åº¦: {data['accuracy']:.4f}" if isinstance(data['accuracy'], float) else f"   ğŸ¯ ç²¾åº¦: {data['accuracy']}")
        print(f"   ğŸ“ é©åˆç‡: {data['precision']:.4f}" if isinstance(data['precision'], float) else f"   ğŸ“ é©åˆç‡: {data['precision']}")
        print(f"   ğŸª å†ç¾ç‡: {data['recall']:.4f}" if isinstance(data['recall'], float) else f"   ğŸª å†ç¾ç‡: {data['recall']}")
        print(f"   âš–ï¸  F1ã‚¹ã‚³ã‚¢: {data['f1_score']:.4f}" if isinstance(data['f1_score'], float) else f"   âš–ï¸  F1ã‚¹ã‚³ã‚¢: {data['f1_score']}")
        print(f"   ğŸ“ˆ ROC AUC: {data['roc_auc']:.4f}" if isinstance(data['roc_auc'], float) else f"   ğŸ“ˆ ROC AUC: {data['roc_auc']}")
        print()
    
    # æœ€é«˜æ€§èƒ½ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š
    if all(isinstance(data['accuracy'], float) for data in comparison_data):
        best_model = max(comparison_data, key=lambda x: x['accuracy'])
        print(f"ğŸ† æœ€é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«: {best_model['name']} (ç²¾åº¦: {best_model['accuracy']:.4f})")
    
    return comparison_data


def load_best_model(directory='.'):
    """
    æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æœ€é«˜æ€§èƒ½ã®ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•é¸æŠã—ã¦èª­ã¿è¾¼ã¿
    
    Parameters:
    -----------
    directory : str
        æ¤œç´¢ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    
    Returns:
    --------
    HorseRacingPredictor or None
        èª­ã¿è¾¼ã¾ã‚ŒãŸäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
    """
    print("ğŸ” æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢ä¸­...")
    
    models_info = list_saved_models(directory)
    if not models_info:
        print("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return None
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    model_files = [info['model_file'].replace('_model.pkl', '.pkl') for info in models_info]
    
    # æ€§èƒ½æ¯”è¼ƒ
    comparison_data = compare_saved_models(model_files)
    if not comparison_data:
        print("âŒ æ€§èƒ½æ¯”è¼ƒã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return None
    
    # æœ€é«˜ç²¾åº¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    best_model = max(comparison_data, key=lambda x: x['accuracy'] if isinstance(x['accuracy'], float) else 0)
    best_model_path = best_model['file'].replace('_model.pkl', '.pkl')
    
    print(f"\nğŸ† æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ: {best_model['name']}")
    print(f"ğŸ“Š ç²¾åº¦: {best_model['accuracy']:.4f}" if isinstance(best_model['accuracy'], float) else f"ğŸ“Š ç²¾åº¦: {best_model['accuracy']}")
    
    # äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    predictor = HorseRacingPredictor()
    success = predictor.load_model(best_model_path)
    
    if success:
        print("âœ… æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†")
        return predictor
    else:
        print("âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
        return None
    TARGET_DATE = '2025/08/02'# '2021/05/30'
    VENUE_CODE = '05'  # æ±äº¬
    DAY_CODE = '0212' 
    
    # 1R~12Rã¾ã§ã®ãƒ¬ãƒ¼ã‚¹IDãƒªã‚¹ãƒˆ
    # race_id_list = [f'2021{VENUE_CODE}{DAY_CODE}{str(i).zfill(2)}' for i in range(1, 13)]
    race_id_list = [f'2025{VENUE_CODE}{DAY_CODE}{str(i).zfill(2)}' for i in range(1, 13)]
    print(f"å¯¾è±¡ãƒ¬ãƒ¼ã‚¹: {race_id_list}")
    
    # å½“æ—¥ã®é¦¬ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    print("\n=== å½“æ—¥ã®é¦¬ãƒ‡ãƒ¼ã‚¿æ›´æ–° ===")
    predictor.update_horse_data(race_id_list, TARGET_DATE)
    
    # 11Rã®æ—¥æœ¬ãƒ€ãƒ¼ãƒ“ãƒ¼ã‚’äºˆæ¸¬ï¼ˆæ•™æä¾‹ï¼‰
    # derby_race_id = f'2021{VENUE_CODE}{DAY_CODE}11'
    derby_race_id = f'2025{VENUE_CODE}{DAY_CODE}11'
    print(f"\n=== {derby_race_id} äºˆæ¸¬å®Ÿè¡Œ ===")
    result = predict_specific_race(derby_race_id, TARGET_DATE)
    
    # å›åç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("\n=== å›åç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ===")
    predictor.simulate_returns()
    
    return predictor


def demo_prediction():
    """ãƒ‡ãƒ¢äºˆæ¸¬ã®å®Ÿè¡Œï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
    print("ğŸ ç«¶é¦¬AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - ãƒ‡ãƒ¢")
    print("=" * 50)
    
    # äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    predictor = HorseRacingPredictor()
    
    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯è¨“ç·´
    # model_path = 'horse_racing_model.pkl'
    model_path = 'model.pkl'
    if Path(model_path).exists():
        print("æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        if not predictor.load_model(model_path):
            print("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã€‚æ–°è¦è¨“ç·´ã‚’é–‹å§‹...")
            if not predictor.train_model():
                print("âŒ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            predictor.save_model(model_path)
    else:
        print("æ–°è¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        if not predictor.train_model():
            print("âŒ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        predictor.save_model(model_path)
    
    print("\nâœ… æº–å‚™å®Œäº†ï¼")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("predictor.predict_race('202408070511')  # ãƒ¬ãƒ¼ã‚¹IDã‚’æŒ‡å®š")
    
    return predictor


if __name__ == "__main__":
    # # Optunaæœ€é©åŒ–ã‚’ä½¿ã£ãŸå½“æ—¥äºˆæ¸¬ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ
    # # n_trials=10ã¯å°‘ãªã„ã§ã™ãŒã€ãƒ‡ãƒ¢ç”¨ã«é«˜é€ŸåŒ–
    # # æœ¬æ ¼é‹ç”¨æ™‚ã¯100ä»¥ä¸Šã«è¨­å®šã—ã¦ãã ã•ã„
    # # predictor = demo_daily_prediction(n_trials=10, use_optuna=True)
    
    # # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¢äºˆæ¸¬ã‚’å®Ÿè¡Œ
    # predictor = demo_prediction()
    # # # äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
    # # predictor = HorseRacingPredictor()
    # # # ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    # # success = predictor.load_model('horse_racing_model.pkl')

    # # if success:
    # if predictor:
    #     print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼äºˆæ¸¬ã‚’é–‹å§‹ã§ãã¾ã™")
    #     # äºˆæ¸¬å®Ÿè¡Œ
    #     results = predictor.predict_race_live('202508070511', '2025/08/07')
    # else:
    #     print("âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")

    
    
    # æ–¹æ³•1: è‡ªå‹•é¸æŠï¼ˆæ¨å¥¨ï¼‰
    predictor = load_best_model()

    # æ–¹æ³•2: ç›´æ¥æŒ‡å®š
    predictor = HorseRacingPredictor()
    predictor.load_model('trained_model_optuna.pkl')

    print("=== æ–°è¦ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬æ¤œè¨¼ ===")
    print("2023å¹´ä»¥é™ã®ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ–°è¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦äºˆæ¸¬ç²¾åº¦ã‚’æ¤œè¨¼ã—ã¾ã™")
    
    # 2023å¹´ä»¥é™ã®å®Ÿéš›ã®ãƒ¬ãƒ¼ã‚¹IDï¼ˆæ¤œè¨¼ç”¨ï¼‰
    validation_races = {
        # 2023å¹´ã®ãƒ¬ãƒ¼ã‚¹
        '202301010101': '2023å¹´1æœˆ1æ—¥ ä¸­å±±1R âœ… å‹•ä½œç¢ºèªæ¸ˆã¿',
        '202301010102': '2023å¹´1æœˆ1æ—¥ ä¸­å±±2R', 
        '202301010103': '2023å¹´1æœˆ1æ—¥ ä¸­å±±3R',
        '202305050501': '2023å¹´5æœˆ5æ—¥ æ±äº¬1R',
        '202305050502': '2023å¹´5æœˆ5æ—¥ æ±äº¬2R',
        '202307150701': '2023å¹´7æœˆ15æ—¥ æ–°æ½Ÿ1R',
        '202310010101': '2023å¹´10æœˆ1æ—¥ ä¸­å±±1R',
        '202312310101': '2023å¹´12æœˆ31æ—¥ ä¸­å±±1R',
        # 2024å¹´ã®ãƒ¬ãƒ¼ã‚¹
        '202401010101': '2024å¹´1æœˆ1æ—¥ ä¸­å±±1R',
        '202401010102': '2024å¹´1æœˆ1æ—¥ ä¸­å±±2R',
        '202401010103': '2024å¹´1æœˆ1æ—¥ ä¸­å±±3R',
        '202404290501': '2024å¹´4æœˆ29æ—¥ æ±äº¬1R',
        '202405050501': '2024å¹´5æœˆ5æ—¥ æ±äº¬1R',
        '202407070701': '2024å¹´7æœˆ7æ—¥ æ–°æ½Ÿ1R',
        '202410010101': '2024å¹´10æœˆ1æ—¥ ä¸­å±±1R',
        '202412310101': '2024å¹´12æœˆ31æ—¥ ä¸­å±±1R',
        # 2025å¹´ã®ãƒ¬ãƒ¼ã‚¹ï¼ˆå®Ÿåœ¨ã™ã‚‹é–‹å‚¬æ—¥ç¨‹ã«åŸºã¥ãï¼‰
        '202501010101': '2025å¹´1æœˆ1æ—¥ ä¸­å±±1R',
        '202501010102': '2025å¹´1æœˆ1æ—¥ ä¸­å±±2R',
        '202501010103': '2025å¹´1æœˆ1æ—¥ ä¸­å±±3R',
        '202502010101': '2025å¹´2æœˆ1æ—¥ ä¸­å±±1R',
        '202503010101': '2025å¹´3æœˆ1æ—¥ ä¸­å±±1R',
        '202504010101': '2025å¹´4æœˆ1æ—¥ ä¸­å±±1R',
        '202505010101': '2025å¹´5æœˆ1æ—¥ ä¸­å±±1R',
        '202506010101': '2025å¹´6æœˆ1æ—¥ ä¸­å±±1R' 
    }
    
    print("\nğŸ‡ æ¤œè¨¼ç”¨ãƒ¬ãƒ¼ã‚¹ï¼ˆ2023-2025å¹´ï¼‰:")
    for i, (race_id, race_name) in enumerate(validation_races.items(), 1):
        if race_id.startswith("2023"):
            year_label = "2023"
        elif race_id.startswith("2024"):
            year_label = "2024"
        else:
            year_label = "2025"
        status = "âœ… å‹•ä½œç¢ºèªæ¸ˆã¿" if race_id == '202301010101' else ""
        print(f"  {i:2d}. {race_id}: {race_name} {status}")
    
    # ãƒ¬ãƒ¼ã‚¹é¸æŠï¼ˆæ‰‹å‹•ã§å¤‰æ›´å¯èƒ½ï¼‰
    # selected_number = 3  # 202301010103ï¼ˆ2023å¹´1æœˆ1æ—¥ ä¸­å±±3Rï¼‰ã‚’ãƒ†ã‚¹ãƒˆ
    # selected_number = 11  # 202501010103ï¼ˆ2025å¹´1æœˆ1æ—¥ ä¸­å±±3Rï¼‰ã‚’ãƒ†ã‚¹ãƒˆ
    selected_number = 23  # 202501010103ï¼ˆ2025å¹´1æœˆ1æ—¥ ä¸­å±±3Rï¼‰ã‚’ãƒ†ã‚¹ãƒˆ
    
    race_ids = list(validation_races.keys())
    selected_race_id = race_ids[selected_number - 1]
    
    print(f"\nğŸ¯ é¸æŠã•ã‚ŒãŸãƒ¬ãƒ¼ã‚¹: {selected_race_id} - {validation_races[selected_race_id]}")
    print(f"â€» ãƒ¬ãƒ¼ã‚¹ã‚’å¤‰æ›´ã™ã‚‹å ´åˆã¯ã€selected_number ã®å€¤ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ï¼ˆ1-{len(validation_races)}ï¼‰")

    # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ
    print(f"\n=== {selected_race_id} ã®äºˆæ¸¬ã‚’é–‹å§‹ ===")
    
    # äºˆæ¸¬å®Ÿè¡Œ
    try:
        results = predictor.predict_race_live(selected_race_id)
        
        if results is not None:
            print("\nğŸ‰ âœ… äºˆæ¸¬å®Œäº†ï¼")
            print("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
            print("ğŸ’¡ 2019-2022å¹´ã®ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã€2023å¹´ä»¥é™ã®æ–°è¦ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã¾ã—ãŸ")
            
            # å®Ÿéš›ã®çµæœã¨ã®æ¯”è¼ƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            print("\nğŸ“‹ å®Ÿéš›ã®ãƒ¬ãƒ¼ã‚¹çµæœã¨ã®æ¯”è¼ƒ:")
            print("â€» æ­£è§£ç‡è¨ˆç®—ã¯æ‰‹å‹•ã§ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
            print(f"  netkeibaã§ç¢ºèª: https://race.netkeiba.com/race/result.html?race_id={selected_race_id}")
            
        else:
            print("âŒ äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ - ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            print("ğŸ’¡ ä»¥ä¸‹ã®ç‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
            print("  1. ãƒ¬ãƒ¼ã‚¹IDãŒæ­£ã—ã„ã‹")
            print("  2. ãƒ¬ãƒ¼ã‚¹ãŒå®Ÿéš›ã«é–‹å‚¬ã•ã‚ŒãŸã‹") 
            print("  3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãŒæ­£å¸¸ã‹")
            print(f"  4. åˆ¥ã®ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’è©¦ã™ï¼ˆselected_number = 1-{len(validation_races)}ï¼‰")
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ğŸ’¡ åˆ¥ã®ãƒ¬ãƒ¼ã‚¹IDã‚’è©¦ã™ã‹ã€selected_number ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„")
    
    print("\nğŸ”§ === ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦ ===")
    print("âœ… æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: å®Œäº†")
    print("âœ… é¦¬ã®éå»æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ: å®Œäº†")  
    print("âœ… è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ: å®Œäº†")
    print("âœ… æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äºˆæ¸¬: å®Œäº†")
    print("âœ… çµæœã®ä¿å­˜ã¨ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: å®Œäº†")
    print("\nğŸ’¡ ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã§ä»»æ„ã®2023å¹´ä»¥é™ã®ãƒ¬ãƒ¼ã‚¹IDã®äºˆæ¸¬ãŒå¯èƒ½ã§ã™")


    # TODO
    # horse_racing_ai_refactored.pyã«å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸäºˆæ¸¬ã‚’å®Ÿè£…ã™ã‚‹
    # webã‚¢ãƒ—ãƒªã«çµ±åˆã™ã‚‹